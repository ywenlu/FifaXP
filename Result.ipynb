{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import stats\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>colere</th>\n",
       "      <th>ennui</th>\n",
       "      <th>fierte</th>\n",
       "      <th>frustration</th>\n",
       "      <th>joie</th>\n",
       "      <th>neutre</th>\n",
       "      <th>peur</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eventidx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          valence  arousal  colere  ennui  fierte  frustration  joie  neutre  \\\n",
       "eventidx                                                                       \n",
       "0               0        1     0.0    0.0     0.0          1.0   0.0     0.0   \n",
       "1               0        1     1.0    0.0     0.0          0.0   0.0     0.0   \n",
       "2               0        1     1.0    0.0     0.0          0.0   0.0     0.0   \n",
       "3               0        0     0.0    0.0     0.0          0.0   0.0     0.0   \n",
       "4               0        1     0.0    0.0     0.0          1.0   0.0     0.0   \n",
       "\n",
       "          peur  \n",
       "eventidx        \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          1.0  \n",
       "4          0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels\n",
    "dfAll = pd.read_csv('./../out/Allevent.txt',encoding='utf-16',index_col=0)\n",
    "eventLabel=pd.DataFrame()\n",
    "eventLabel['eventidx']=dfAll.index\n",
    "eventLabel['valence']=dfAll['valence'].map(lambda x: 1 if x >0 else 0)\n",
    "eventLabel['arousal']=dfAll['arousal'].map(lambda x: 1 if x >1 else 0)\n",
    "eventLabel.set_index('eventidx',inplace=True)\n",
    "\n",
    "\n",
    "eventdf=pd.get_dummies(dfAll.event)\n",
    "emodf=pd.get_dummies(dfAll.emotions)\n",
    "\n",
    "eventLabel=pd.concat([eventLabel,emodf],axis=1)\n",
    "eventLabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phyEventdf10=pd.read_csv('./../out/feature/phyEvent10s.csv',encoding='utf-16',index_col='eventidx')\n",
    "phyEventdf14=pd.read_csv('./../out/feature/phyEvent14s.csv',encoding='utf-16',index_col='eventidx')\n",
    "phyEventdf20=pd.read_csv('./../out/feature/phyEvent20s.csv',encoding='utf-16',index_col='eventidx')\n",
    "phyEventdf30=pd.read_csv('./../out/feature/phyEvent30s.csv',encoding='utf-16',index_col='eventidx')\n",
    "\n",
    "dfLens=[phyEventdf10,phyEventdf14,phyEventdf20,phyEventdf30]\n",
    "(featureTrans10,featureTrans14,featureTrans20,featureTrans30)=[df.groupby('sujet').transform(lambda x: (x - x.mean()) / x.std()) for df in dfLens]\n",
    "\n",
    "phyEventNormdf10=pd.concat([phyEventdf10['sujet'], featureTrans10], axis=1)\n",
    "phyEventNormdf14=pd.concat([phyEventdf14['sujet'], featureTrans14], axis=1)\n",
    "phyEventNormdf20=pd.concat([phyEventdf20['sujet'], featureTrans20], axis=1)\n",
    "phyEventNormdf30=pd.concat([phyEventdf30['sujet'], featureTrans30], axis=1)\n",
    "\n",
    "phyEventNormdf10n=phyEventNormdf10.dropna(axis=1,how='any')\n",
    "phyEventNormdf14n=phyEventNormdf14.dropna(axis=1,how='any')\n",
    "phyEventNormdf20n=phyEventNormdf20.dropna(axis=1,how='any')\n",
    "phyEventNormdf30n=phyEventNormdf30.dropna(axis=1,how='any')\n",
    "\n",
    "DF={'10s':phyEventNormdf10n,\n",
    "     '14s':phyEventNormdf14n,\n",
    "     '20s':phyEventNormdf20n,\n",
    "     '30s':phyEventNormdf30n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "10\n",
      "  valence\n",
      "     RF\n",
      "     dummy\n",
      "  arousal\n",
      "     RF\n",
      "     dummy\n",
      "  colere\n",
      "     RF\n",
      "     dummy\n",
      "  ennui\n",
      "     RF\n",
      "     dummy\n",
      "  fierte\n",
      "     RF\n",
      "     dummy\n",
      "  frustration\n",
      "     RF\n",
      "     dummy\n",
      "  joie\n",
      "     RF\n",
      "     dummy\n",
      "  neutre\n",
      "     RF\n",
      "     dummy\n",
      "  peur\n",
      "     RF\n",
      "     dummy\n",
      "=============\n",
      "30\n",
      "  valence\n",
      "     RF\n",
      "     dummy\n",
      "  arousal\n",
      "     RF\n",
      "     dummy\n",
      "  colere\n",
      "     RF\n",
      "     dummy\n",
      "  ennui\n",
      "     RF\n",
      "     dummy\n",
      "  fierte\n",
      "     RF\n",
      "     dummy\n",
      "  frustration\n",
      "     RF\n",
      "     dummy\n",
      "  joie\n",
      "     RF\n",
      "     dummy\n",
      "  neutre\n",
      "     RF\n",
      "     dummy\n",
      "  peur\n",
      "     RF\n",
      "     dummy\n",
      "=============\n",
      "20\n",
      "  valence\n",
      "     RF\n",
      "     dummy\n",
      "  arousal\n",
      "     RF\n",
      "     dummy\n",
      "  colere\n",
      "     RF\n",
      "     dummy\n",
      "  ennui\n",
      "     RF\n",
      "     dummy\n",
      "  fierte\n",
      "     RF\n",
      "     dummy\n",
      "  frustration\n",
      "     RF\n",
      "     dummy\n",
      "  joie\n",
      "     RF\n",
      "     dummy\n",
      "  neutre\n",
      "     RF\n",
      "     dummy\n",
      "  peur\n",
      "     RF\n",
      "     dummy\n",
      "=============\n",
      "14\n",
      "  valence\n",
      "     RF\n",
      "     dummy\n",
      "  arousal\n",
      "     RF\n",
      "     dummy\n",
      "  colere\n",
      "     RF\n",
      "     dummy\n",
      "  ennui\n",
      "     RF\n",
      "     dummy\n",
      "  fierte\n",
      "     RF\n",
      "     dummy\n",
      "  frustration\n",
      "     RF\n",
      "     dummy\n",
      "  joie\n",
      "     RF\n",
      "     dummy\n",
      "  neutre\n",
      "     RF\n",
      "     dummy\n",
      "  peur\n",
      "     RF\n",
      "     dummy\n"
     ]
    }
   ],
   "source": [
    "#apprentissage\n",
    "def meanstd(scores):\n",
    "    return \"{0:.3f} ({1:.3f})\".format(scores.mean(),np.std(scores))\n",
    "\n",
    "scoresls=[\"accuracy\",\"f1\"]\n",
    "names=['RF','dummy']\n",
    "classifiers=[RandomForestClassifier(max_depth=5, n_estimators=50),DummyClassifier(strategy='stratified',random_state=0)]\n",
    "\n",
    "cmpclassifierdf=pd.DataFrame()\n",
    "for DFname, X in DF.iteritems():\n",
    "    print(\"=============\")\n",
    "    print(DFname)\n",
    "    Xn=X.dropna(axis=1,how='any')\n",
    "    y_all=eventLabel.ix[18:,:]\n",
    "    X_y=pd.concat([Xn,y_all],axis=1,join='inner')\n",
    "    for label in list((eventLabel.columns.values).astype(str)):\n",
    "        print('  '+label)\n",
    "        y=X_y.loc[:,label]\n",
    "        Xn=X_y.iloc[:,:-9]\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            print('     '+name)\n",
    "            scores_a = cross_val_score(clf, Xn, y, scoring=\"accuracy\",cv=10)\n",
    "            scores_f = cross_val_score(clf, Xn, y, scoring=\"f1\",cv=10)\n",
    "            ncmp=pd.DataFrame({'DFname':DFname,'label':label,'clf':name,'accuracy':meanstd(scores_a),'F1':meanstd(scores_f) }, index=[0])\n",
    "            cmpclassifierdf=cmpclassifierdf.append(ncmp)\n",
    "\n",
    "\n",
    "    #print(\"{0:.3f} (std: {1:.3f})\".format(scores.mean(),np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DFname</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>clf</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.695 (0.002)</td>\n",
       "      <td>RF</td>\n",
       "      <td>valence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>valence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.394 (0.130)</td>\n",
       "      <td>0.475 (0.067)</td>\n",
       "      <td>RF</td>\n",
       "      <td>arousal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>arousal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.012 (0.035)</td>\n",
       "      <td>0.826 (0.003)</td>\n",
       "      <td>RF</td>\n",
       "      <td>colere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>colere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>RF</td>\n",
       "      <td>ennui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>ennui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>RF</td>\n",
       "      <td>fierte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>fierte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.037 (0.083)</td>\n",
       "      <td>0.627 (0.037)</td>\n",
       "      <td>RF</td>\n",
       "      <td>frustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>frustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>RF</td>\n",
       "      <td>joie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>joie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>RF</td>\n",
       "      <td>neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.007 (0.022)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>RF</td>\n",
       "      <td>peur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>dummy</td>\n",
       "      <td>peur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DFname             F1       accuracy    clf        label\n",
       "0     30  0.000 (0.000)  0.695 (0.002)     RF      valence\n",
       "0     30  0.270 (0.065)  0.566 (0.038)  dummy      valence\n",
       "0     30  0.394 (0.130)  0.475 (0.067)     RF      arousal\n",
       "0     30  0.476 (0.029)  0.463 (0.029)  dummy      arousal\n",
       "0     30  0.012 (0.035)  0.826 (0.003)     RF       colere\n",
       "0     30  0.213 (0.059)  0.731 (0.020)  dummy       colere\n",
       "0     30  0.000 (0.000)  0.979 (0.003)     RF        ennui\n",
       "0     30  0.000 (0.000)  0.965 (0.000)  dummy        ennui\n",
       "0     30  0.000 (0.000)  0.922 (0.002)     RF       fierte\n",
       "0     30  0.058 (0.071)  0.850 (0.011)  dummy       fierte\n",
       "0     30  0.037 (0.083)  0.627 (0.037)     RF  frustration\n",
       "0     30  0.385 (0.022)  0.552 (0.016)  dummy  frustration\n",
       "0     30  0.000 (0.000)  0.832 (0.000)     RF         joie\n",
       "0     30  0.168 (0.054)  0.726 (0.018)  dummy         joie\n",
       "0     30  0.000 (0.000)  0.949 (0.002)     RF       neutre\n",
       "0     30  0.063 (0.052)  0.898 (0.005)  dummy       neutre\n",
       "0     30  0.007 (0.022)  0.852 (0.002)     RF         peur\n",
       "0     30  0.153 (0.047)  0.757 (0.014)  dummy         peur"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmpclassifierdf[cmpclassifierdf['DFname']=='30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DFname</th>\n",
       "      <th>clf</th>\n",
       "      <th>label</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>RF</td>\n",
       "      <td>valence</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>dummy</td>\n",
       "      <td>valence</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.275 (0.052)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>RF</td>\n",
       "      <td>arousal</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.370 (0.098)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>dummy</td>\n",
       "      <td>arousal</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.479 (0.033)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>RF</td>\n",
       "      <td>colere</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DFname    clf    label variable          value\n",
       "0     10     RF  valence       F1  0.000 (0.000)\n",
       "1     10  dummy  valence       F1  0.275 (0.052)\n",
       "2     10     RF  arousal       F1  0.370 (0.098)\n",
       "3     10  dummy  arousal       F1  0.479 (0.033)\n",
       "4     10     RF   colere       F1  0.000 (0.000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp=pd.melt(cmpclassifierdf, id_vars=['DFname','clf','label'], value_vars=['F1', 'accuracy'])\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-feb48be583b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DFname'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'variable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/pandas/tools/pivot.pyc\u001b[0m in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0magged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3595\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelectionMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3597\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3599\u001b[0m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0m_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3114\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/pandas/core/base.pyc\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0m_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \"\"\"\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, numeric_only)\u001b[0m\n\u001b[1;32m   3046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m         new_items, new_blocks = self._cython_agg_blocks(\n\u001b[0;32m-> 3048\u001b[0;31m             how, numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m   3049\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, numeric_only)\u001b[0m\n\u001b[1;32m   3092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3094\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No numeric types to aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "pd.pivot_table(tmp, values='value', index=['DFname','clf'], columns=['label','variable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "10\n",
      " ECG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " fEMG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " EDA.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " Resp.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "=============\n",
      "30\n",
      " ECG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " fEMG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " EDA.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " Resp.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "=============\n",
      "20\n",
      " ECG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " fEMG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " EDA.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " Resp.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "=============\n",
      "14\n",
      " ECG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " fEMG.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " EDA.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      " Resp.*\n",
      "  valence\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  arousal\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  colere\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  ennui\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  fierte\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  frustration\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  joie\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  neutre\n",
      "    DummyClassifier\n",
      "    Random Forest\n",
      "  peur\n",
      "    DummyClassifier\n",
      "    Random Forest\n"
     ]
    }
   ],
   "source": [
    "feapattern=[\"ECG.*\",\"fEMG.*\",\"EDA.*\",\"Resp.*\"]\n",
    "\n",
    "names = [\"DummyClassifier\",\n",
    "         \"Random Forest\"]\n",
    "classifiers = [\n",
    "    DummyClassifier(strategy='stratified',random_state=0),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=50)\n",
    "    ]\n",
    "\n",
    "feanames=X.columns.values\n",
    "\n",
    "cmpclassifierdf=pd.DataFrame()\n",
    "for DFname, X in DF.iteritems():\n",
    "    print(\"=============\")\n",
    "    print(DFname)\n",
    "    Xn=X.dropna(axis=1,how='any')\n",
    "    y_all=eventLabel.ix[18:,:]\n",
    "    X_y=pd.concat([Xn,y_all],axis=1,join='inner')\n",
    "    for pattern in feapattern:\n",
    "        print(\" \"+pattern)\n",
    "        regexb=re.compile(pattern)\n",
    "        selectf=[m.group(0) for l in feanames for m in [regexb.search(l)] if m]\n",
    "        Xn_s=X_y[list(set(selectf) & set(X_y.columns.values))]\n",
    "        for label in list((eventLabel.columns.values).astype(str)):\n",
    "            print('  '+label)\n",
    "            y=X_y.loc[:,label]\n",
    "            for name, clf in zip(names, classifiers):\n",
    "                print('    '+name)\n",
    "                scores_a = cross_val_score(clf, Xn, y, scoring=\"accuracy\",cv=10)\n",
    "                scores_f = cross_val_score(clf, Xn, y, scoring=\"f1\",cv=10)\n",
    "                ncmp=pd.DataFrame({'DFname':DFname,'signal':pattern,'label':label,'clf':name,'accuracy':meanstd(scores_a),'F1':meanstd(scores_f) }, index=[0])\n",
    "                cmpclassifierdf=cmpclassifierdf.append(ncmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DFname</th>\n",
       "      <th>clf</th>\n",
       "      <th>label</th>\n",
       "      <th>signal</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>valence</td>\n",
       "      <td>ECG.*</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.275 (0.052)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>valence</td>\n",
       "      <td>ECG.*</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>arousal</td>\n",
       "      <td>ECG.*</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.479 (0.033)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>arousal</td>\n",
       "      <td>ECG.*</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.395 (0.118)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>colere</td>\n",
       "      <td>ECG.*</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.213 (0.037)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DFname              clf    label signal variable          value\n",
       "0     10  DummyClassifier  valence  ECG.*       F1  0.275 (0.052)\n",
       "1     10    Random Forest  valence  ECG.*       F1  0.000 (0.000)\n",
       "2     10  DummyClassifier  arousal  ECG.*       F1  0.479 (0.033)\n",
       "3     10    Random Forest  arousal  ECG.*       F1  0.395 (0.118)\n",
       "4     10  DummyClassifier   colere  ECG.*       F1  0.213 (0.037)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2=pd.melt(cmpclassifierdf, id_vars=['DFname','clf','label','signal'], value_vars=['F1', 'accuracy'])\n",
    "tmp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"18\" halign=\"left\">value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th colspan=\"2\" halign=\"left\">arousal</th>\n",
       "      <th colspan=\"2\" halign=\"left\">colere</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ennui</th>\n",
       "      <th colspan=\"2\" halign=\"left\">fierte</th>\n",
       "      <th colspan=\"2\" halign=\"left\">frustration</th>\n",
       "      <th colspan=\"2\" halign=\"left\">joie</th>\n",
       "      <th colspan=\"2\" halign=\"left\">neutre</th>\n",
       "      <th colspan=\"2\" halign=\"left\">peur</th>\n",
       "      <th colspan=\"2\" halign=\"left\">valence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DFname</th>\n",
       "      <th>signal</th>\n",
       "      <th>clf</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ECG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.479 (0.033)</td>\n",
       "      <td>0.466 (0.033)</td>\n",
       "      <td>0.213 (0.037)</td>\n",
       "      <td>0.730 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.051 (0.057)</td>\n",
       "      <td>0.850 (0.009)</td>\n",
       "      <td>0.370 (0.034)</td>\n",
       "      <td>0.540 (0.024)</td>\n",
       "      <td>0.143 (0.048)</td>\n",
       "      <td>0.722 (0.015)</td>\n",
       "      <td>0.053 (0.053)</td>\n",
       "      <td>0.896 (0.006)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.275 (0.052)</td>\n",
       "      <td>0.569 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.395 (0.118)</td>\n",
       "      <td>0.452 (0.083)</td>\n",
       "      <td>0.029 (0.086)</td>\n",
       "      <td>0.823 (0.005)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.068 (0.144)</td>\n",
       "      <td>0.626 (0.041)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EDA.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.479 (0.033)</td>\n",
       "      <td>0.466 (0.033)</td>\n",
       "      <td>0.213 (0.037)</td>\n",
       "      <td>0.730 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.051 (0.057)</td>\n",
       "      <td>0.850 (0.009)</td>\n",
       "      <td>0.370 (0.034)</td>\n",
       "      <td>0.540 (0.024)</td>\n",
       "      <td>0.143 (0.048)</td>\n",
       "      <td>0.722 (0.015)</td>\n",
       "      <td>0.053 (0.053)</td>\n",
       "      <td>0.896 (0.006)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.275 (0.052)</td>\n",
       "      <td>0.569 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.387 (0.099)</td>\n",
       "      <td>0.477 (0.059)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.005)</td>\n",
       "      <td>0.009 (0.019)</td>\n",
       "      <td>0.632 (0.027)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.695 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Resp.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.479 (0.033)</td>\n",
       "      <td>0.466 (0.033)</td>\n",
       "      <td>0.213 (0.037)</td>\n",
       "      <td>0.730 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.051 (0.057)</td>\n",
       "      <td>0.850 (0.009)</td>\n",
       "      <td>0.370 (0.034)</td>\n",
       "      <td>0.540 (0.024)</td>\n",
       "      <td>0.143 (0.048)</td>\n",
       "      <td>0.722 (0.015)</td>\n",
       "      <td>0.053 (0.053)</td>\n",
       "      <td>0.896 (0.006)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.275 (0.052)</td>\n",
       "      <td>0.569 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.370 (0.118)</td>\n",
       "      <td>0.455 (0.058)</td>\n",
       "      <td>0.006 (0.019)</td>\n",
       "      <td>0.823 (0.007)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.005)</td>\n",
       "      <td>0.037 (0.080)</td>\n",
       "      <td>0.634 (0.021)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fEMG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.479 (0.033)</td>\n",
       "      <td>0.466 (0.033)</td>\n",
       "      <td>0.213 (0.037)</td>\n",
       "      <td>0.730 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.051 (0.057)</td>\n",
       "      <td>0.850 (0.009)</td>\n",
       "      <td>0.370 (0.034)</td>\n",
       "      <td>0.540 (0.024)</td>\n",
       "      <td>0.143 (0.048)</td>\n",
       "      <td>0.722 (0.015)</td>\n",
       "      <td>0.053 (0.053)</td>\n",
       "      <td>0.896 (0.006)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.275 (0.052)</td>\n",
       "      <td>0.569 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.389 (0.126)</td>\n",
       "      <td>0.464 (0.059)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.818 (0.021)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.978 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.033 (0.069)</td>\n",
       "      <td>0.637 (0.009)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">14</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ECG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.477 (0.032)</td>\n",
       "      <td>0.464 (0.033)</td>\n",
       "      <td>0.216 (0.037)</td>\n",
       "      <td>0.731 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.065 (0.068)</td>\n",
       "      <td>0.851 (0.011)</td>\n",
       "      <td>0.362 (0.040)</td>\n",
       "      <td>0.535 (0.030)</td>\n",
       "      <td>0.123 (0.045)</td>\n",
       "      <td>0.711 (0.015)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.257 (0.068)</td>\n",
       "      <td>0.557 (0.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.373 (0.126)</td>\n",
       "      <td>0.464 (0.052)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.003)</td>\n",
       "      <td>0.020 (0.061)</td>\n",
       "      <td>0.634 (0.021)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.007 (0.015)</td>\n",
       "      <td>0.693 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EDA.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.477 (0.032)</td>\n",
       "      <td>0.464 (0.033)</td>\n",
       "      <td>0.216 (0.037)</td>\n",
       "      <td>0.731 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.065 (0.068)</td>\n",
       "      <td>0.851 (0.011)</td>\n",
       "      <td>0.362 (0.040)</td>\n",
       "      <td>0.535 (0.030)</td>\n",
       "      <td>0.123 (0.045)</td>\n",
       "      <td>0.711 (0.015)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.257 (0.068)</td>\n",
       "      <td>0.557 (0.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.379 (0.117)</td>\n",
       "      <td>0.456 (0.073)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.799 (0.078)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.016 (0.047)</td>\n",
       "      <td>0.635 (0.017)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Resp.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.477 (0.032)</td>\n",
       "      <td>0.464 (0.033)</td>\n",
       "      <td>0.216 (0.037)</td>\n",
       "      <td>0.731 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.065 (0.068)</td>\n",
       "      <td>0.851 (0.011)</td>\n",
       "      <td>0.362 (0.040)</td>\n",
       "      <td>0.535 (0.030)</td>\n",
       "      <td>0.123 (0.045)</td>\n",
       "      <td>0.711 (0.015)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.257 (0.068)</td>\n",
       "      <td>0.557 (0.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.364 (0.152)</td>\n",
       "      <td>0.475 (0.053)</td>\n",
       "      <td>0.006 (0.019)</td>\n",
       "      <td>0.812 (0.040)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.003)</td>\n",
       "      <td>0.030 (0.070)</td>\n",
       "      <td>0.635 (0.008)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fEMG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.477 (0.032)</td>\n",
       "      <td>0.464 (0.033)</td>\n",
       "      <td>0.216 (0.037)</td>\n",
       "      <td>0.731 (0.013)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.065 (0.068)</td>\n",
       "      <td>0.851 (0.011)</td>\n",
       "      <td>0.362 (0.040)</td>\n",
       "      <td>0.535 (0.030)</td>\n",
       "      <td>0.123 (0.045)</td>\n",
       "      <td>0.711 (0.015)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.161 (0.057)</td>\n",
       "      <td>0.759 (0.017)</td>\n",
       "      <td>0.257 (0.068)</td>\n",
       "      <td>0.557 (0.040)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.380 (0.112)</td>\n",
       "      <td>0.471 (0.056)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.023 (0.068)</td>\n",
       "      <td>0.627 (0.043)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">20</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ECG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.382 (0.146)</td>\n",
       "      <td>0.475 (0.059)</td>\n",
       "      <td>0.024 (0.071)</td>\n",
       "      <td>0.788 (0.113)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.050 (0.130)</td>\n",
       "      <td>0.640 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.851 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EDA.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.381 (0.145)</td>\n",
       "      <td>0.459 (0.059)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.016 (0.049)</td>\n",
       "      <td>0.639 (0.007)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.693 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Resp.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.375 (0.157)</td>\n",
       "      <td>0.458 (0.057)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.824 (0.006)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.003)</td>\n",
       "      <td>0.026 (0.058)</td>\n",
       "      <td>0.634 (0.027)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fEMG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.366 (0.144)</td>\n",
       "      <td>0.484 (0.059)</td>\n",
       "      <td>0.009 (0.028)</td>\n",
       "      <td>0.826 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.045 (0.125)</td>\n",
       "      <td>0.635 (0.011)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">30</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ECG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.388 (0.119)</td>\n",
       "      <td>0.469 (0.055)</td>\n",
       "      <td>0.006 (0.019)</td>\n",
       "      <td>0.824 (0.004)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.021 (0.045)</td>\n",
       "      <td>0.639 (0.011)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EDA.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.405 (0.109)</td>\n",
       "      <td>0.482 (0.085)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.826 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.018 (0.023)</td>\n",
       "      <td>0.635 (0.010)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Resp.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.365 (0.114)</td>\n",
       "      <td>0.478 (0.060)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.824 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.032 (0.062)</td>\n",
       "      <td>0.637 (0.006)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.692 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fEMG.*</th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.476 (0.029)</td>\n",
       "      <td>0.463 (0.029)</td>\n",
       "      <td>0.213 (0.059)</td>\n",
       "      <td>0.731 (0.020)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.965 (0.000)</td>\n",
       "      <td>0.058 (0.071)</td>\n",
       "      <td>0.850 (0.011)</td>\n",
       "      <td>0.385 (0.022)</td>\n",
       "      <td>0.552 (0.016)</td>\n",
       "      <td>0.168 (0.054)</td>\n",
       "      <td>0.726 (0.018)</td>\n",
       "      <td>0.063 (0.052)</td>\n",
       "      <td>0.898 (0.005)</td>\n",
       "      <td>0.153 (0.047)</td>\n",
       "      <td>0.757 (0.014)</td>\n",
       "      <td>0.270 (0.065)</td>\n",
       "      <td>0.566 (0.038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.373 (0.123)</td>\n",
       "      <td>0.474 (0.074)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.021 (0.029)</td>\n",
       "      <td>0.637 (0.010)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       value                                \\\n",
       "label                                arousal                        colere   \n",
       "variable                                  F1       accuracy             F1   \n",
       "DFname signal clf                                                            \n",
       "10     ECG.*  DummyClassifier  0.479 (0.033)  0.466 (0.033)  0.213 (0.037)   \n",
       "              Random Forest    0.395 (0.118)  0.452 (0.083)  0.029 (0.086)   \n",
       "       EDA.*  DummyClassifier  0.479 (0.033)  0.466 (0.033)  0.213 (0.037)   \n",
       "              Random Forest    0.387 (0.099)  0.477 (0.059)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.479 (0.033)  0.466 (0.033)  0.213 (0.037)   \n",
       "              Random Forest    0.370 (0.118)  0.455 (0.058)  0.006 (0.019)   \n",
       "       fEMG.* DummyClassifier  0.479 (0.033)  0.466 (0.033)  0.213 (0.037)   \n",
       "              Random Forest    0.389 (0.126)  0.464 (0.059)  0.000 (0.000)   \n",
       "14     ECG.*  DummyClassifier  0.477 (0.032)  0.464 (0.033)  0.216 (0.037)   \n",
       "              Random Forest    0.373 (0.126)  0.464 (0.052)  0.000 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.477 (0.032)  0.464 (0.033)  0.216 (0.037)   \n",
       "              Random Forest    0.379 (0.117)  0.456 (0.073)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.477 (0.032)  0.464 (0.033)  0.216 (0.037)   \n",
       "              Random Forest    0.364 (0.152)  0.475 (0.053)  0.006 (0.019)   \n",
       "       fEMG.* DummyClassifier  0.477 (0.032)  0.464 (0.033)  0.216 (0.037)   \n",
       "              Random Forest    0.380 (0.112)  0.471 (0.056)  0.000 (0.000)   \n",
       "20     ECG.*  DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.382 (0.146)  0.475 (0.059)  0.024 (0.071)   \n",
       "       EDA.*  DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.381 (0.145)  0.459 (0.059)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.375 (0.157)  0.458 (0.057)  0.000 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.366 (0.144)  0.484 (0.059)  0.009 (0.028)   \n",
       "30     ECG.*  DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.388 (0.119)  0.469 (0.055)  0.006 (0.019)   \n",
       "       EDA.*  DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.405 (0.109)  0.482 (0.085)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.365 (0.114)  0.478 (0.060)  0.000 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.476 (0.029)  0.463 (0.029)  0.213 (0.059)   \n",
       "              Random Forest    0.373 (0.123)  0.474 (0.074)  0.000 (0.000)   \n",
       "\n",
       "                                                                            \\\n",
       "label                                                 ennui                  \n",
       "variable                            accuracy             F1       accuracy   \n",
       "DFname signal clf                                                            \n",
       "10     ECG.*  DummyClassifier  0.730 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.823 (0.005)  0.000 (0.000)  0.979 (0.003)   \n",
       "       EDA.*  DummyClassifier  0.730 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.825 (0.002)  0.000 (0.000)  0.979 (0.003)   \n",
       "       Resp.* DummyClassifier  0.730 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.823 (0.007)  0.000 (0.000)  0.979 (0.003)   \n",
       "       fEMG.* DummyClassifier  0.730 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.818 (0.021)  0.000 (0.000)  0.978 (0.003)   \n",
       "14     ECG.*  DummyClassifier  0.731 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.825 (0.002)  0.000 (0.000)  0.979 (0.003)   \n",
       "       EDA.*  DummyClassifier  0.731 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.799 (0.078)  0.000 (0.000)  0.979 (0.003)   \n",
       "       Resp.* DummyClassifier  0.731 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.812 (0.040)  0.000 (0.000)  0.979 (0.003)   \n",
       "       fEMG.* DummyClassifier  0.731 (0.013)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.825 (0.002)  0.000 (0.000)  0.979 (0.003)   \n",
       "20     ECG.*  DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.788 (0.113)  0.000 (0.000)  0.979 (0.003)   \n",
       "       EDA.*  DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.825 (0.002)  0.000 (0.000)  0.979 (0.003)   \n",
       "       Resp.* DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.824 (0.006)  0.000 (0.000)  0.979 (0.003)   \n",
       "       fEMG.* DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.826 (0.003)  0.000 (0.000)  0.979 (0.003)   \n",
       "30     ECG.*  DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.824 (0.004)  0.000 (0.000)  0.979 (0.003)   \n",
       "       EDA.*  DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.826 (0.003)  0.000 (0.000)  0.979 (0.003)   \n",
       "       Resp.* DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.824 (0.003)  0.000 (0.000)  0.979 (0.003)   \n",
       "       fEMG.* DummyClassifier  0.731 (0.020)  0.000 (0.000)  0.965 (0.000)   \n",
       "              Random Forest    0.825 (0.002)  0.000 (0.000)  0.979 (0.003)   \n",
       "\n",
       "                                                                            \\\n",
       "label                                 fierte                   frustration   \n",
       "variable                                  F1       accuracy             F1   \n",
       "DFname signal clf                                                            \n",
       "10     ECG.*  DummyClassifier  0.051 (0.057)  0.850 (0.009)  0.370 (0.034)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.003)  0.068 (0.144)   \n",
       "       EDA.*  DummyClassifier  0.051 (0.057)  0.850 (0.009)  0.370 (0.034)   \n",
       "              Random Forest    0.000 (0.000)  0.921 (0.005)  0.009 (0.019)   \n",
       "       Resp.* DummyClassifier  0.051 (0.057)  0.850 (0.009)  0.370 (0.034)   \n",
       "              Random Forest    0.000 (0.000)  0.921 (0.005)  0.037 (0.080)   \n",
       "       fEMG.* DummyClassifier  0.051 (0.057)  0.850 (0.009)  0.370 (0.034)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.003)  0.033 (0.069)   \n",
       "14     ECG.*  DummyClassifier  0.065 (0.068)  0.851 (0.011)  0.362 (0.040)   \n",
       "              Random Forest    0.000 (0.000)  0.921 (0.003)  0.020 (0.061)   \n",
       "       EDA.*  DummyClassifier  0.065 (0.068)  0.851 (0.011)  0.362 (0.040)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.003)  0.016 (0.047)   \n",
       "       Resp.* DummyClassifier  0.065 (0.068)  0.851 (0.011)  0.362 (0.040)   \n",
       "              Random Forest    0.000 (0.000)  0.921 (0.003)  0.030 (0.070)   \n",
       "       fEMG.* DummyClassifier  0.065 (0.068)  0.851 (0.011)  0.362 (0.040)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.003)  0.023 (0.068)   \n",
       "20     ECG.*  DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.002)  0.050 (0.130)   \n",
       "       EDA.*  DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.002)  0.016 (0.049)   \n",
       "       Resp.* DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.921 (0.003)  0.026 (0.058)   \n",
       "       fEMG.* DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.002)  0.045 (0.125)   \n",
       "30     ECG.*  DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.002)  0.021 (0.045)   \n",
       "       EDA.*  DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.002)  0.018 (0.023)   \n",
       "       Resp.* DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.002)  0.032 (0.062)   \n",
       "       fEMG.* DummyClassifier  0.058 (0.071)  0.850 (0.011)  0.385 (0.022)   \n",
       "              Random Forest    0.000 (0.000)  0.922 (0.002)  0.021 (0.029)   \n",
       "\n",
       "                                                                            \\\n",
       "label                                                  joie                  \n",
       "variable                            accuracy             F1       accuracy   \n",
       "DFname signal clf                                                            \n",
       "10     ECG.*  DummyClassifier  0.540 (0.024)  0.143 (0.048)  0.722 (0.015)   \n",
       "              Random Forest    0.626 (0.041)  0.000 (0.000)  0.833 (0.001)   \n",
       "       EDA.*  DummyClassifier  0.540 (0.024)  0.143 (0.048)  0.722 (0.015)   \n",
       "              Random Forest    0.632 (0.027)  0.000 (0.000)  0.833 (0.001)   \n",
       "       Resp.* DummyClassifier  0.540 (0.024)  0.143 (0.048)  0.722 (0.015)   \n",
       "              Random Forest    0.634 (0.021)  0.000 (0.000)  0.833 (0.001)   \n",
       "       fEMG.* DummyClassifier  0.540 (0.024)  0.143 (0.048)  0.722 (0.015)   \n",
       "              Random Forest    0.637 (0.009)  0.000 (0.000)  0.833 (0.001)   \n",
       "14     ECG.*  DummyClassifier  0.535 (0.030)  0.123 (0.045)  0.711 (0.015)   \n",
       "              Random Forest    0.634 (0.021)  0.000 (0.000)  0.832 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.535 (0.030)  0.123 (0.045)  0.711 (0.015)   \n",
       "              Random Forest    0.635 (0.017)  0.000 (0.000)  0.832 (0.000)   \n",
       "       Resp.* DummyClassifier  0.535 (0.030)  0.123 (0.045)  0.711 (0.015)   \n",
       "              Random Forest    0.635 (0.008)  0.000 (0.000)  0.832 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.535 (0.030)  0.123 (0.045)  0.711 (0.015)   \n",
       "              Random Forest    0.627 (0.043)  0.000 (0.000)  0.832 (0.000)   \n",
       "20     ECG.*  DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.640 (0.003)  0.000 (0.000)  0.832 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.639 (0.007)  0.000 (0.000)  0.832 (0.002)   \n",
       "       Resp.* DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.634 (0.027)  0.000 (0.000)  0.832 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.635 (0.011)  0.000 (0.000)  0.832 (0.000)   \n",
       "30     ECG.*  DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.639 (0.011)  0.000 (0.000)  0.832 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.635 (0.010)  0.000 (0.000)  0.832 (0.000)   \n",
       "       Resp.* DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.637 (0.006)  0.000 (0.000)  0.832 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.552 (0.016)  0.168 (0.054)  0.726 (0.018)   \n",
       "              Random Forest    0.637 (0.010)  0.000 (0.000)  0.832 (0.000)   \n",
       "\n",
       "                                                                            \\\n",
       "label                                 neutre                          peur   \n",
       "variable                                  F1       accuracy             F1   \n",
       "DFname signal clf                                                            \n",
       "10     ECG.*  DummyClassifier  0.053 (0.053)  0.896 (0.006)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.948 (0.002)  0.000 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.053 (0.053)  0.896 (0.006)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.948 (0.002)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.053 (0.053)  0.896 (0.006)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.948 (0.003)  0.000 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.053 (0.053)  0.896 (0.006)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.948 (0.002)  0.000 (0.000)   \n",
       "14     ECG.*  DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.161 (0.057)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "20     ECG.*  DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "30     ECG.*  DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       EDA.*  DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       Resp.* DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "       fEMG.* DummyClassifier  0.063 (0.052)  0.898 (0.005)  0.153 (0.047)   \n",
       "              Random Forest    0.000 (0.000)  0.949 (0.002)  0.000 (0.000)   \n",
       "\n",
       "                                                                            \n",
       "label                                               valence                 \n",
       "variable                            accuracy             F1       accuracy  \n",
       "DFname signal clf                                                           \n",
       "10     ECG.*  DummyClassifier  0.759 (0.017)  0.275 (0.052)  0.569 (0.031)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.694 (0.003)  \n",
       "       EDA.*  DummyClassifier  0.759 (0.017)  0.275 (0.052)  0.569 (0.031)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.695 (0.003)  \n",
       "       Resp.* DummyClassifier  0.759 (0.017)  0.275 (0.052)  0.569 (0.031)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.694 (0.004)  \n",
       "       fEMG.* DummyClassifier  0.759 (0.017)  0.275 (0.052)  0.569 (0.031)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.694 (0.003)  \n",
       "14     ECG.*  DummyClassifier  0.759 (0.017)  0.257 (0.068)  0.557 (0.040)  \n",
       "              Random Forest    0.852 (0.002)  0.007 (0.015)  0.693 (0.004)  \n",
       "       EDA.*  DummyClassifier  0.759 (0.017)  0.257 (0.068)  0.557 (0.040)  \n",
       "              Random Forest    0.852 (0.002)  0.004 (0.011)  0.693 (0.003)  \n",
       "       Resp.* DummyClassifier  0.759 (0.017)  0.257 (0.068)  0.557 (0.040)  \n",
       "              Random Forest    0.852 (0.002)  0.004 (0.011)  0.693 (0.003)  \n",
       "       fEMG.* DummyClassifier  0.759 (0.017)  0.257 (0.068)  0.557 (0.040)  \n",
       "              Random Forest    0.852 (0.002)  0.004 (0.011)  0.693 (0.003)  \n",
       "20     ECG.*  DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.851 (0.002)  0.004 (0.011)  0.694 (0.001)  \n",
       "       EDA.*  DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.693 (0.004)  \n",
       "       Resp.* DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.852 (0.002)  0.004 (0.011)  0.694 (0.001)  \n",
       "       fEMG.* DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.852 (0.002)  0.004 (0.011)  0.693 (0.002)  \n",
       "30     ECG.*  DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.693 (0.003)  \n",
       "       EDA.*  DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.852 (0.002)  0.004 (0.011)  0.694 (0.001)  \n",
       "       Resp.* DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.692 (0.004)  \n",
       "       fEMG.* DummyClassifier  0.757 (0.014)  0.270 (0.065)  0.566 (0.038)  \n",
       "              Random Forest    0.852 (0.002)  0.000 (0.000)  0.694 (0.001)  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2.set_index(['DFname','signal','clf','variable','label']).unstack().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"18\" halign=\"left\">value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th colspan=\"2\" halign=\"left\">arousal</th>\n",
       "      <th colspan=\"2\" halign=\"left\">colere</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ennui</th>\n",
       "      <th colspan=\"2\" halign=\"left\">fierte</th>\n",
       "      <th colspan=\"2\" halign=\"left\">frustration</th>\n",
       "      <th colspan=\"2\" halign=\"left\">joie</th>\n",
       "      <th colspan=\"2\" halign=\"left\">neutre</th>\n",
       "      <th colspan=\"2\" halign=\"left\">peur</th>\n",
       "      <th colspan=\"2\" halign=\"left\">valence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DFname</th>\n",
       "      <th>signal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">10</th>\n",
       "      <th>ECG.*</th>\n",
       "      <td>0.395 (0.118)</td>\n",
       "      <td>0.452 (0.083)</td>\n",
       "      <td>0.029 (0.086)</td>\n",
       "      <td>0.823 (0.005)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.068 (0.144)</td>\n",
       "      <td>0.626 (0.041)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDA.*</th>\n",
       "      <td>0.387 (0.099)</td>\n",
       "      <td>0.477 (0.059)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.005)</td>\n",
       "      <td>0.009 (0.019)</td>\n",
       "      <td>0.632 (0.027)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.695 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resp.*</th>\n",
       "      <td>0.370 (0.118)</td>\n",
       "      <td>0.455 (0.058)</td>\n",
       "      <td>0.006 (0.019)</td>\n",
       "      <td>0.823 (0.007)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.005)</td>\n",
       "      <td>0.037 (0.080)</td>\n",
       "      <td>0.634 (0.021)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fEMG.*</th>\n",
       "      <td>0.389 (0.126)</td>\n",
       "      <td>0.464 (0.059)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.818 (0.021)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.978 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.033 (0.069)</td>\n",
       "      <td>0.637 (0.009)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.833 (0.001)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.948 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">14</th>\n",
       "      <th>ECG.*</th>\n",
       "      <td>0.373 (0.126)</td>\n",
       "      <td>0.464 (0.052)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.003)</td>\n",
       "      <td>0.020 (0.061)</td>\n",
       "      <td>0.634 (0.021)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.007 (0.015)</td>\n",
       "      <td>0.693 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDA.*</th>\n",
       "      <td>0.379 (0.117)</td>\n",
       "      <td>0.456 (0.073)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.799 (0.078)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.016 (0.047)</td>\n",
       "      <td>0.635 (0.017)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resp.*</th>\n",
       "      <td>0.364 (0.152)</td>\n",
       "      <td>0.475 (0.053)</td>\n",
       "      <td>0.006 (0.019)</td>\n",
       "      <td>0.812 (0.040)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.003)</td>\n",
       "      <td>0.030 (0.070)</td>\n",
       "      <td>0.635 (0.008)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fEMG.*</th>\n",
       "      <td>0.380 (0.112)</td>\n",
       "      <td>0.471 (0.056)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.003)</td>\n",
       "      <td>0.023 (0.068)</td>\n",
       "      <td>0.627 (0.043)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">20</th>\n",
       "      <th>ECG.*</th>\n",
       "      <td>0.382 (0.146)</td>\n",
       "      <td>0.475 (0.059)</td>\n",
       "      <td>0.024 (0.071)</td>\n",
       "      <td>0.788 (0.113)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.050 (0.130)</td>\n",
       "      <td>0.640 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.851 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDA.*</th>\n",
       "      <td>0.381 (0.145)</td>\n",
       "      <td>0.459 (0.059)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.016 (0.049)</td>\n",
       "      <td>0.639 (0.007)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.693 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resp.*</th>\n",
       "      <td>0.375 (0.157)</td>\n",
       "      <td>0.458 (0.057)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.824 (0.006)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.921 (0.003)</td>\n",
       "      <td>0.026 (0.058)</td>\n",
       "      <td>0.634 (0.027)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fEMG.*</th>\n",
       "      <td>0.366 (0.144)</td>\n",
       "      <td>0.484 (0.059)</td>\n",
       "      <td>0.009 (0.028)</td>\n",
       "      <td>0.826 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.045 (0.125)</td>\n",
       "      <td>0.635 (0.011)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.693 (0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">30</th>\n",
       "      <th>ECG.*</th>\n",
       "      <td>0.388 (0.119)</td>\n",
       "      <td>0.469 (0.055)</td>\n",
       "      <td>0.006 (0.019)</td>\n",
       "      <td>0.824 (0.004)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.021 (0.045)</td>\n",
       "      <td>0.639 (0.011)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.693 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDA.*</th>\n",
       "      <td>0.405 (0.109)</td>\n",
       "      <td>0.482 (0.085)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.826 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.018 (0.023)</td>\n",
       "      <td>0.635 (0.010)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.004 (0.011)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resp.*</th>\n",
       "      <td>0.365 (0.114)</td>\n",
       "      <td>0.478 (0.060)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.824 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.032 (0.062)</td>\n",
       "      <td>0.637 (0.006)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.692 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fEMG.*</th>\n",
       "      <td>0.373 (0.123)</td>\n",
       "      <td>0.474 (0.074)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.825 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.979 (0.003)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.922 (0.002)</td>\n",
       "      <td>0.021 (0.029)</td>\n",
       "      <td>0.637 (0.010)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.832 (0.000)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.949 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.852 (0.002)</td>\n",
       "      <td>0.000 (0.000)</td>\n",
       "      <td>0.694 (0.001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       value                                               \\\n",
       "label                arousal                        colere                  \n",
       "variable                  F1       accuracy             F1       accuracy   \n",
       "DFname signal                                                               \n",
       "10     ECG.*   0.395 (0.118)  0.452 (0.083)  0.029 (0.086)  0.823 (0.005)   \n",
       "       EDA.*   0.387 (0.099)  0.477 (0.059)  0.000 (0.000)  0.825 (0.002)   \n",
       "       Resp.*  0.370 (0.118)  0.455 (0.058)  0.006 (0.019)  0.823 (0.007)   \n",
       "       fEMG.*  0.389 (0.126)  0.464 (0.059)  0.000 (0.000)  0.818 (0.021)   \n",
       "14     ECG.*   0.373 (0.126)  0.464 (0.052)  0.000 (0.000)  0.825 (0.002)   \n",
       "       EDA.*   0.379 (0.117)  0.456 (0.073)  0.000 (0.000)  0.799 (0.078)   \n",
       "       Resp.*  0.364 (0.152)  0.475 (0.053)  0.006 (0.019)  0.812 (0.040)   \n",
       "       fEMG.*  0.380 (0.112)  0.471 (0.056)  0.000 (0.000)  0.825 (0.002)   \n",
       "20     ECG.*   0.382 (0.146)  0.475 (0.059)  0.024 (0.071)  0.788 (0.113)   \n",
       "       EDA.*   0.381 (0.145)  0.459 (0.059)  0.000 (0.000)  0.825 (0.002)   \n",
       "       Resp.*  0.375 (0.157)  0.458 (0.057)  0.000 (0.000)  0.824 (0.006)   \n",
       "       fEMG.*  0.366 (0.144)  0.484 (0.059)  0.009 (0.028)  0.826 (0.003)   \n",
       "30     ECG.*   0.388 (0.119)  0.469 (0.055)  0.006 (0.019)  0.824 (0.004)   \n",
       "       EDA.*   0.405 (0.109)  0.482 (0.085)  0.000 (0.000)  0.826 (0.003)   \n",
       "       Resp.*  0.365 (0.114)  0.478 (0.060)  0.000 (0.000)  0.824 (0.003)   \n",
       "       fEMG.*  0.373 (0.123)  0.474 (0.074)  0.000 (0.000)  0.825 (0.002)   \n",
       "\n",
       "                                                                           \\\n",
       "label                  ennui                        fierte                  \n",
       "variable                  F1       accuracy             F1       accuracy   \n",
       "DFname signal                                                               \n",
       "10     ECG.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.003)   \n",
       "       EDA.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.921 (0.005)   \n",
       "       Resp.*  0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.921 (0.005)   \n",
       "       fEMG.*  0.000 (0.000)  0.978 (0.003)  0.000 (0.000)  0.922 (0.003)   \n",
       "14     ECG.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.921 (0.003)   \n",
       "       EDA.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.003)   \n",
       "       Resp.*  0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.921 (0.003)   \n",
       "       fEMG.*  0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.003)   \n",
       "20     ECG.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.002)   \n",
       "       EDA.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.002)   \n",
       "       Resp.*  0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.921 (0.003)   \n",
       "       fEMG.*  0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.002)   \n",
       "30     ECG.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.002)   \n",
       "       EDA.*   0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.002)   \n",
       "       Resp.*  0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.002)   \n",
       "       fEMG.*  0.000 (0.000)  0.979 (0.003)  0.000 (0.000)  0.922 (0.002)   \n",
       "\n",
       "                                                                           \\\n",
       "label            frustration                          joie                  \n",
       "variable                  F1       accuracy             F1       accuracy   \n",
       "DFname signal                                                               \n",
       "10     ECG.*   0.068 (0.144)  0.626 (0.041)  0.000 (0.000)  0.833 (0.001)   \n",
       "       EDA.*   0.009 (0.019)  0.632 (0.027)  0.000 (0.000)  0.833 (0.001)   \n",
       "       Resp.*  0.037 (0.080)  0.634 (0.021)  0.000 (0.000)  0.833 (0.001)   \n",
       "       fEMG.*  0.033 (0.069)  0.637 (0.009)  0.000 (0.000)  0.833 (0.001)   \n",
       "14     ECG.*   0.020 (0.061)  0.634 (0.021)  0.000 (0.000)  0.832 (0.000)   \n",
       "       EDA.*   0.016 (0.047)  0.635 (0.017)  0.000 (0.000)  0.832 (0.000)   \n",
       "       Resp.*  0.030 (0.070)  0.635 (0.008)  0.000 (0.000)  0.832 (0.000)   \n",
       "       fEMG.*  0.023 (0.068)  0.627 (0.043)  0.000 (0.000)  0.832 (0.000)   \n",
       "20     ECG.*   0.050 (0.130)  0.640 (0.003)  0.000 (0.000)  0.832 (0.000)   \n",
       "       EDA.*   0.016 (0.049)  0.639 (0.007)  0.000 (0.000)  0.832 (0.002)   \n",
       "       Resp.*  0.026 (0.058)  0.634 (0.027)  0.000 (0.000)  0.832 (0.000)   \n",
       "       fEMG.*  0.045 (0.125)  0.635 (0.011)  0.000 (0.000)  0.832 (0.000)   \n",
       "30     ECG.*   0.021 (0.045)  0.639 (0.011)  0.000 (0.000)  0.832 (0.000)   \n",
       "       EDA.*   0.018 (0.023)  0.635 (0.010)  0.000 (0.000)  0.832 (0.000)   \n",
       "       Resp.*  0.032 (0.062)  0.637 (0.006)  0.000 (0.000)  0.832 (0.000)   \n",
       "       fEMG.*  0.021 (0.029)  0.637 (0.010)  0.000 (0.000)  0.832 (0.000)   \n",
       "\n",
       "                                                                           \\\n",
       "label                 neutre                          peur                  \n",
       "variable                  F1       accuracy             F1       accuracy   \n",
       "DFname signal                                                               \n",
       "10     ECG.*   0.000 (0.000)  0.948 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       EDA.*   0.000 (0.000)  0.948 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       Resp.*  0.000 (0.000)  0.948 (0.003)  0.000 (0.000)  0.852 (0.002)   \n",
       "       fEMG.*  0.000 (0.000)  0.948 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "14     ECG.*   0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       EDA.*   0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       Resp.*  0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       fEMG.*  0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "20     ECG.*   0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.851 (0.002)   \n",
       "       EDA.*   0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       Resp.*  0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       fEMG.*  0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "30     ECG.*   0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       EDA.*   0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       Resp.*  0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "       fEMG.*  0.000 (0.000)  0.949 (0.002)  0.000 (0.000)  0.852 (0.002)   \n",
       "\n",
       "                                             \n",
       "label                valence                 \n",
       "variable                  F1       accuracy  \n",
       "DFname signal                                \n",
       "10     ECG.*   0.000 (0.000)  0.694 (0.003)  \n",
       "       EDA.*   0.000 (0.000)  0.695 (0.003)  \n",
       "       Resp.*  0.000 (0.000)  0.694 (0.004)  \n",
       "       fEMG.*  0.000 (0.000)  0.694 (0.003)  \n",
       "14     ECG.*   0.007 (0.015)  0.693 (0.004)  \n",
       "       EDA.*   0.004 (0.011)  0.693 (0.003)  \n",
       "       Resp.*  0.004 (0.011)  0.693 (0.003)  \n",
       "       fEMG.*  0.004 (0.011)  0.693 (0.003)  \n",
       "20     ECG.*   0.004 (0.011)  0.694 (0.001)  \n",
       "       EDA.*   0.000 (0.000)  0.693 (0.004)  \n",
       "       Resp.*  0.004 (0.011)  0.694 (0.001)  \n",
       "       fEMG.*  0.004 (0.011)  0.693 (0.002)  \n",
       "30     ECG.*   0.000 (0.000)  0.693 (0.003)  \n",
       "       EDA.*   0.004 (0.011)  0.694 (0.001)  \n",
       "       Resp.*  0.000 (0.000)  0.692 (0.004)  \n",
       "       fEMG.*  0.000 (0.000)  0.694 (0.001)  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2[tmp2['clf']=='Random Forest'].drop('clf',axis=1).set_index(['DFname','signal','variable','label']).unstack().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phyEventdf10=pd.read_csv('./../out/feature/phyEvent10s.csv',encoding='utf-16',index_col='eventidx')\n",
    "phyEventdf14=pd.read_csv('./../out/feature/phyEvent14s.csv',encoding='utf-16',index_col='eventidx')\n",
    "phyEventdf20=pd.read_csv('./../out/feature/phyEvent20s.csv',encoding='utf-16',index_col='eventidx')\n",
    "phyEventdf30=pd.read_csv('./../out/feature/phyEvent30s.csv',encoding='utf-16',index_col='eventidx')\n",
    "\n",
    "dfraw=[phyEventdf10,phyEventdf14,phyEventdf20,phyEventdf30]\n",
    "\n",
    "trans_std=[df.groupby('sujet').transform(lambda x: (x - x.mean()) / x.std()) for df in dfraw]\n",
    "trans_range=[df.groupby('sujet').transform(lambda x: x*1.0/(x.max()-x.min())) for df in dfraw]\n",
    "\n",
    "df_std=[pd.concat([dfraw[i]['sujet'],trans_std[i]],axis=1) for i in range(4)]\n",
    "df_range=[pd.concat([dfraw[i]['sujet'],trans_range[i]],axis=1) for i in range(4)]\n",
    "\n",
    "df_rawn= [df.dropna(axis=1,how='any') for df in dfraw]\n",
    "df_stdn= [df.dropna(axis=1,how='any') for df in df_std]\n",
    "df_rangen= [df.dropna(axis=1,how='any') for df in df_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[          sujet  ECG_raw_mean  ECG_raw_median  ECG_raw_max  ECG_raw_min  \\\n",
       " eventidx                                                                  \n",
       " 18            2      0.100631       -0.019883    -1.613760     0.541414   \n",
       " 19            2     -0.043344       -1.269493    -1.867755     0.629894   \n",
       " 20            2      0.002886       -0.748208    -1.980017     0.585579   \n",
       " 21            2      0.000323       -0.168964    -1.930547     0.644569   \n",
       " 22            2     -0.245557       -0.781633    -1.776856     0.597419   \n",
       " 23            2     -0.042863       -0.575061    -1.386609     0.590685   \n",
       " 24            2     -1.893077       -0.177028    -1.721504    -2.128112   \n",
       " 25            2      0.184833       -0.952139    -1.846245     0.555634   \n",
       " 26            2     -0.114418       -1.586056    -1.504301     0.582842   \n",
       " 27            2      0.374219        0.452028     0.049539     0.317282   \n",
       " 28            2      0.285585        0.013617    -0.132779     0.354099   \n",
       " 29            2     -0.008732        0.242905     0.290411    -0.173997   \n",
       " 30            2      1.153774        0.482112     0.733770     0.308988   \n",
       " 31            2      0.290347        0.106419     0.082153     0.335646   \n",
       " 32            2      0.061585        0.807465     0.047409     0.272433   \n",
       " 33            2      0.059314        0.790473    -0.029412     0.347397   \n",
       " 34            2      0.244877        0.922191    -0.104759     0.348381   \n",
       " 35            2     -0.198227       -1.324726    -0.487156     0.326017   \n",
       " 36            2     -0.038406       -0.623834    -0.256395     0.417904   \n",
       " 37            2      0.959301       -0.473816    -0.302561     0.396345   \n",
       " 38            2     -1.296888        0.881075    -0.326618    -1.018304   \n",
       " 39            2     -0.392243       -0.957294    -0.306267     0.012852   \n",
       " 40            2     -0.354038       -2.128729     0.058636     0.331744   \n",
       " 41            2      0.103796       -0.405123     0.154303     0.252099   \n",
       " 42            2     -0.071179       -3.037959     0.245845     0.305507   \n",
       " 43            2     -1.347175       -0.588804     0.698317    -0.841731   \n",
       " 44            2     -0.011994       -1.080170     0.366829     0.166886   \n",
       " 45            2      0.064344        1.703415     0.639320     0.126851   \n",
       " 46            2      1.097497        1.017756     0.448418     0.101824   \n",
       " 47            2      0.181448        0.928385     0.431720     0.185785   \n",
       " ...         ...           ...             ...          ...          ...   \n",
       " 1718         58     -0.621149        1.433274    -0.597458     0.218102   \n",
       " 1719         58      2.416515        2.263875     0.952670     0.113677   \n",
       " 1720         58      0.002480       -0.337232     3.123059    -4.317597   \n",
       " 1721         58      1.364519        0.803058    -0.191194     0.337329   \n",
       " 1722         58     -0.579614        0.956263    -0.910271    -0.139434   \n",
       " 1723         58     -0.531820        0.938495     0.674210    -1.211213   \n",
       " 1724         58     -0.758182        0.778611    -0.205265    -0.125204   \n",
       " 1725         58     -0.510852       -0.770559    -0.711430     0.663945   \n",
       " 1726         58     -0.260186        0.442757    -1.040148     0.895320   \n",
       " 1727         58     -0.344588        0.208537    -0.797248     0.254757   \n",
       " 1728         58      0.139564       -0.116427    -0.794862     0.456186   \n",
       " 1729         58     -0.577164        0.894104    -0.840574     0.793240   \n",
       " 1730         58      0.371534        0.653524    -0.690510     0.360017   \n",
       " 1731         58     -0.246022        0.536506    -1.013471     1.028310   \n",
       " 1732         58     -0.155486        0.387258    -0.686584     0.965081   \n",
       " 1733         58     -1.482510       -1.192941    -0.221360     0.450081   \n",
       " 1734         58     -0.427114        0.507763    -0.488008     0.754123   \n",
       " 1735         58      0.655815        0.193593     0.046917     0.163247   \n",
       " 1736         58     -0.249104       -0.316428    -0.105839     0.512871   \n",
       " 1737         58     -0.529457       -0.762775    -0.050212    -0.015613   \n",
       " 1738         58      1.990251        1.216249    -0.199591     0.136148   \n",
       " 1739         58     -0.735186       -0.323142     0.303852    -0.209063   \n",
       " 1740         58     -0.102909       -0.236363    -0.382517     0.059451   \n",
       " 1741         58     -0.981209       -1.222294    -0.451417    -0.368446   \n",
       " 1742         58     -0.570678       -1.078330     1.529458    -1.557828   \n",
       " 1743         58     -0.233354       -1.296383     0.702397    -0.617377   \n",
       " 1744         58      0.536082       -1.526018     0.238283    -0.237987   \n",
       " 1745         58     -0.083965       -2.151809     0.212035    -0.109529   \n",
       " 1746         58     -0.517230       -0.901493    -0.115945     0.364129   \n",
       " 1747         58      3.021020        0.018326     2.711026     0.383276   \n",
       " \n",
       "           ECG_raw_vrange  ECG_raw_var  ECG_raw_stddev  ECG_raw_avgder  \\\n",
       " eventidx                                                                \n",
       " 18             -1.245434    -1.580337       -1.581311        0.033481   \n",
       " 19             -1.444044    -1.638585       -1.642882       -0.030844   \n",
       " 20             -1.465733    -1.701671       -1.709781       -0.187995   \n",
       " 21             -1.487441    -2.277986       -2.331642        0.046555   \n",
       " 22             -1.372324    -2.165089       -2.208257        0.541280   \n",
       " 23             -1.169545    -1.868618       -1.887910       -0.053593   \n",
       " 24              0.815095    -1.737016       -1.747360        3.184353   \n",
       " 25             -1.374326    -1.442140       -1.435981       -0.082356   \n",
       " 26             -1.222877    -1.813189       -1.828591        0.048124   \n",
       " 27             -0.226316    -0.055541       -0.031778        0.476171   \n",
       " 28             -0.347729     0.560552        0.563694       -0.202116   \n",
       " 29              0.284789     0.348317        0.360380       -1.603923   \n",
       " 30              0.126441     0.747942        0.741674        0.049431   \n",
       " 31             -0.224364     0.282024        0.296488        0.339415   \n",
       " 32             -0.191859    -0.177629       -0.151736        0.157946   \n",
       " 33             -0.290120    -0.356789       -0.328992       -0.021692   \n",
       " 34             -0.329022     0.212657        0.229433       -0.090201   \n",
       " 35             -0.504775     0.519416        0.524433       -0.469351   \n",
       " 36             -0.460824     0.096933        0.117108       -0.161585   \n",
       " 37             -0.467101    -0.204782       -0.178507       -0.592247   \n",
       " 38              0.641541    -0.351158       -0.323398        2.420820   \n",
       " 39             -0.165137     0.325714        0.338617        1.836919   \n",
       " 40             -0.233171     0.858801        0.846303       -0.301217   \n",
       " 41             -0.121666     0.375262        0.386297        0.105650   \n",
       " 42             -0.117666     0.602218        0.603390       -0.370772   \n",
       " 43              1.020207     1.358640        1.312140        1.836926   \n",
       " 44              0.053374     0.315044        0.328335       -0.027445   \n",
       " 45              0.222959    -0.064287       -0.040350       -0.151388   \n",
       " 46              0.146202     0.820345        0.810063       -0.612905   \n",
       " 47              0.071232    -0.089742       -0.065315       -0.080264   \n",
       " ...                  ...          ...             ...             ...   \n",
       " 1718           -0.516220    -0.987061       -0.997808        0.158034   \n",
       " 1719            0.680065    -1.008623       -1.020388       -1.170046   \n",
       " 1720            3.683572     1.607098        1.596233       -1.096695   \n",
       " 1721           -0.248144     0.041490        0.058698       -0.750394   \n",
       " 1722           -0.640243    -1.171271       -1.191309        0.392193   \n",
       " 1723            0.881755    -0.927352       -0.935378        0.841468   \n",
       " 1724           -0.115304    -0.021924       -0.005315       -0.725000   \n",
       " 1725           -0.740025    -0.420517       -0.410986        0.517029   \n",
       " 1726           -1.058578    -0.490130       -0.482433        0.990994   \n",
       " 1727           -0.677597    -0.713854       -0.713287       -0.174162   \n",
       " 1728           -0.738256    -0.338076       -0.326605       -1.287832   \n",
       " 1729           -0.877080    -1.091635       -1.107490        0.344234   \n",
       " 1730           -0.630088    -0.892179       -0.898668        0.719455   \n",
       " 1731           -1.079780    -1.145735       -1.164403       -1.029688   \n",
       " 1732           -0.814734    -1.229050       -1.252282        0.586149   \n",
       " 1733           -0.305751     0.136992        0.154834       -1.594633   \n",
       " 1734           -0.600229    -0.762489       -0.763724        0.784340   \n",
       " 1735           -0.015385     0.377900        0.395929        2.596254   \n",
       " 1736           -0.238480    -0.561576       -0.555950       -0.107155   \n",
       " 1737           -0.032861     0.488039        0.505491       -0.461924   \n",
       " 1738           -0.192074     0.991298        1.000971        0.928219   \n",
       " 1739            0.292965     1.461134        1.456200       -1.100220   \n",
       " 1740           -0.305644    -0.497529       -0.490038       -0.870997   \n",
       " 1741           -0.224712     0.510589        0.527871       -0.460512   \n",
       " 1742            1.631380     1.586432        1.576446        0.857691   \n",
       " 1743            0.718804     0.849913        0.862610        1.239260   \n",
       " 1744            0.252700     1.835278        1.813856       -0.513412   \n",
       " 1745            0.193164     2.156467        2.117579        1.497400   \n",
       " 1746           -0.199952    -0.152816       -0.137896        0.121355   \n",
       " 1747            1.916733     0.369188        0.387245       -1.231407   \n",
       " \n",
       "           ECG_raw_maxgra         ...           Resp_one_Num  Resp_Ampup_mean  \\\n",
       " eventidx                         ...                                           \n",
       " 18             -1.609814         ...               0.145865        -0.555752   \n",
       " 19             -1.624434         ...               0.145865        -0.672117   \n",
       " 20             -1.715546         ...               0.145865        -0.444246   \n",
       " 21             -1.694291         ...               0.145865        -0.929960   \n",
       " 22             -1.406056         ...               0.145865        -1.017488   \n",
       " 23             -1.200748         ...               0.145865        -0.884538   \n",
       " 24             -1.354898         ...               0.145865        -0.902662   \n",
       " 25             -1.889861         ...               0.145865        -0.731401   \n",
       " 26             -1.282196         ...              -6.709790        -0.730596   \n",
       " 27              0.107037         ...               0.145865        -0.146873   \n",
       " 28             -0.004708         ...               0.145865        -0.660249   \n",
       " 29              4.256083         ...               0.145865        -0.861819   \n",
       " 30              0.115611         ...               0.145865        -0.640476   \n",
       " 31              0.183646         ...               0.145865        -0.904985   \n",
       " 32              0.043723         ...               0.145865        -0.349749   \n",
       " 33             -0.263434         ...               0.145865        -0.311059   \n",
       " 34             -0.059552         ...               0.145865        -0.135529   \n",
       " 35             -0.285748         ...               0.145865         0.154539   \n",
       " 36             -0.219936         ...               0.145865        -0.461895   \n",
       " 37             -0.117188         ...               0.145865        -0.784859   \n",
       " 38             -0.253373         ...               0.145865        -0.856243   \n",
       " 39             -0.227404         ...               0.145865        -0.882462   \n",
       " 40              0.197969         ...               0.145865        -0.335681   \n",
       " 41              0.394664         ...               0.145865        -0.548716   \n",
       " 42              0.456299         ...               0.145865        -0.695641   \n",
       " 43              0.845000         ...               0.145865        -0.660226   \n",
       " 44              0.181519         ...               0.145865         1.073035   \n",
       " 45              0.222684         ...               0.145865         0.623272   \n",
       " 46              0.032322         ...               0.145865         0.403148   \n",
       " 47              0.155443         ...               0.145865        -0.032606   \n",
       " ...                  ...         ...                    ...              ...   \n",
       " 1718           -0.955181         ...              -0.385640        -0.432338   \n",
       " 1719           -0.844716         ...              -0.385640        -0.696144   \n",
       " 1720            2.902698         ...              -0.385640         0.241455   \n",
       " 1721           -0.026694         ...              -0.385640        -0.980213   \n",
       " 1722           -1.577902         ...               2.506658         0.053983   \n",
       " 1723            0.158096         ...              -0.385640         1.196127   \n",
       " 1724           -0.858284         ...              -0.385640        -0.025677   \n",
       " 1725            2.966740         ...               2.506658         0.765708   \n",
       " 1726           -0.813091         ...              -0.385640         0.275811   \n",
       " 1727           -0.755292         ...               2.506658         0.868502   \n",
       " 1728           -0.998177         ...              -0.385640        -0.477335   \n",
       " 1729           -0.516015         ...              -0.385640         0.057028   \n",
       " 1730           -0.661875         ...              -0.385640         0.508058   \n",
       " 1731           -0.661875         ...              -0.385640         0.855288   \n",
       " 1732           -0.534139         ...              -0.385640         0.574480   \n",
       " 1733           -0.034490         ...              -0.385640         0.262120   \n",
       " 1734           -0.135358         ...              -0.385640        -0.017255   \n",
       " 1735            0.286253         ...              -0.385640         0.147296   \n",
       " 1736           -0.165093         ...              -0.385640        -0.672426   \n",
       " 1737            0.016783         ...              -0.385640         3.056444   \n",
       " 1738            0.062065         ...              -0.385640         1.103588   \n",
       " 1739            0.384682         ...              -0.385640        -0.441461   \n",
       " 1740           -0.440156         ...              -0.385640        -0.138036   \n",
       " 1741           -0.436795         ...              -0.385640         0.495918   \n",
       " 1742            0.945145         ...              -0.385640         0.652311   \n",
       " 1743            0.696671         ...              -0.385640        -1.348706   \n",
       " 1744            0.471830         ...              -0.385640        -1.310805   \n",
       " 1745            0.859452         ...               2.506658        -1.815452   \n",
       " 1746            0.252136         ...              -0.385640        -1.003297   \n",
       " 1747            0.412581         ...              -0.385640        -1.754971   \n",
       " \n",
       "           Resp_Ampup_max  Resp_Ampup_min  Resp_Ampup_vrange  \\\n",
       " eventidx                                                      \n",
       " 18             -0.665869       -0.551059          -0.600726   \n",
       " 19             -0.684047       -0.676562          -0.555672   \n",
       " 20             -0.505459       -0.115566          -0.624436   \n",
       " 21             -0.966542       -0.699170          -0.928018   \n",
       " 22             -0.963571       -1.131216          -0.683604   \n",
       " 23             -0.861589       -0.916126          -0.664309   \n",
       " 24             -0.909343       -0.709740          -0.844199   \n",
       " 25             -0.795354       -0.667133          -0.712584   \n",
       " 26             -0.702563       -0.614522          -0.615417   \n",
       " 27             -0.177549        0.053027          -0.271426   \n",
       " 28             -0.759770       -0.433486          -0.794085   \n",
       " 29             -0.818233       -1.138975          -0.481252   \n",
       " 30             -0.732300       -0.315751          -0.822155   \n",
       " 31             -0.848299       -0.898450          -0.656034   \n",
       " 32             -0.519540        0.131118          -0.780863   \n",
       " 33             -0.536223        0.286438          -0.890006   \n",
       " 34              0.177865       -0.200246           0.353762   \n",
       " 35              0.264088        0.377664           0.149733   \n",
       " 36             -0.507562       -0.589452          -0.363659   \n",
       " 37             -0.648436       -1.033298          -0.308682   \n",
       " 38             -0.925317       -0.540292          -0.960237   \n",
       " 39             -0.910729       -0.644213          -0.882543   \n",
       " 40             -0.259613       -0.327954          -0.171291   \n",
       " 41             -0.337696       -0.832244           0.002872   \n",
       " 42             -0.602242       -0.721832          -0.419021   \n",
       " 43             -0.644958       -0.985434          -0.330573   \n",
       " 44              1.384116       -0.850669           2.359237   \n",
       " 45              0.799939        0.099176           1.034808   \n",
       " 46              0.507655        0.097584           0.637433   \n",
       " 47             -0.191844        0.506470          -0.543174   \n",
       " ...                  ...             ...                ...   \n",
       " 1718           -0.827168        0.302877          -1.040120   \n",
       " 1719           -0.381373       -0.863685           0.111043   \n",
       " 1720            0.527274        0.262771           0.394695   \n",
       " 1721           -0.521537       -1.678730           0.444773   \n",
       " 1722            0.143158       -0.344974           0.352223   \n",
       " 1723            2.902534       -0.822377           3.508263   \n",
       " 1724            0.002215        0.419513          -0.244634   \n",
       " 1725            0.356526        1.290552          -0.388202   \n",
       " 1726            0.009772       -0.137058           0.090860   \n",
       " 1727            0.735931        0.737737           0.332512   \n",
       " 1728            0.434338       -0.740367           0.888351   \n",
       " 1729           -0.200822       -0.550314           0.114698   \n",
       " 1730           -0.135266        0.510755          -0.441585   \n",
       " 1731            0.051806        1.466793          -0.809436   \n",
       " 1732            0.913266        0.411980           0.709034   \n",
       " 1733           -0.304820        0.887946          -0.840275   \n",
       " 1734           -0.501885        0.498278          -0.816225   \n",
       " 1735            0.245181        0.645969          -0.124786   \n",
       " 1736            0.275596       -0.918658           0.827905   \n",
       " 1737            2.156563        2.160115           0.975415   \n",
       " 1738            0.523710        1.162458          -0.138610   \n",
       " 1739           -0.785674       -0.034200          -0.798470   \n",
       " 1740           -0.749954        0.800850          -1.252797   \n",
       " 1741           -0.044717        0.877618          -0.563192   \n",
       " 1742            1.559706       -0.555969           1.952339   \n",
       " 1743           -1.322506       -0.811258          -0.900394   \n",
       " 1744           -1.327855       -0.920986          -0.841377   \n",
       " 1745           -1.058475       -2.352366           0.281861   \n",
       " 1746           -1.029386       -0.616996          -0.709339   \n",
       " 1747           -1.646140       -1.088274          -1.074529   \n",
       " \n",
       "           Resp_Ampup_avgder  Resp_Ampup_maxgra  Resp_Ampup_absdev  \\\n",
       " eventidx                                                            \n",
       " 18                 0.056861          -0.252174          -0.609067   \n",
       " 19                -0.062889          -0.190058          -0.501747   \n",
       " 20                 0.082343          -0.267128          -0.629054   \n",
       " 21                -0.230912          -0.631763          -0.821763   \n",
       " 22                -0.061889          -0.333443          -0.668905   \n",
       " 23                 0.141096          -0.391654          -0.656227   \n",
       " 24                -0.158209          -0.520795          -0.704458   \n",
       " 25                -0.459134          -0.636635          -0.664949   \n",
       " 26                -0.122226          -0.257019          -0.610607   \n",
       " 27                -0.065859           0.128522          -0.301372   \n",
       " 28                -0.389014          -0.629099          -0.722727   \n",
       " 29                 0.063303          -0.106649          -0.495005   \n",
       " 30                -0.275997          -0.531347          -0.691220   \n",
       " 31                -0.208142          -0.359396          -0.483934   \n",
       " 32                -0.077537          -0.525095          -0.701104   \n",
       " 33                -0.302974          -0.619492          -0.810040   \n",
       " 34                 1.200934           0.829226           0.037589   \n",
       " 35                 0.083858           0.464912           0.110186   \n",
       " 36                -0.735881          -0.713859          -0.276049   \n",
       " 37                 0.027070           0.086766          -0.400115   \n",
       " 38                -0.199056          -0.657049          -0.857902   \n",
       " 39                -0.211650          -0.556411          -0.802454   \n",
       " 40                 0.707793          -0.082471          -0.320235   \n",
       " 41                -0.384923          -0.158330           0.121802   \n",
       " 42                -0.250054          -0.041574          -0.169844   \n",
       " 43                 0.511714           0.062231          -0.428786   \n",
       " 44                 2.328709           1.139793           2.516997   \n",
       " 45                -1.807521          -1.404395           0.990374   \n",
       " 46                 0.193306           1.147162           0.533056   \n",
       " 47                -0.349890          -0.326745          -0.567219   \n",
       " ...                     ...                ...                ...   \n",
       " 1718              -0.104329          -0.928238          -1.059251   \n",
       " 1719               0.636386          -0.157336           0.021058   \n",
       " 1720               0.469612           0.831663           0.303120   \n",
       " 1721               0.032340           1.249277           0.314468   \n",
       " 1722               0.815761           0.157236           0.864723   \n",
       " 1723               3.163022           3.234901           2.994119   \n",
       " 1724              -0.664538           0.255720          -0.222080   \n",
       " 1725              -0.555235          -0.365788          -0.347318   \n",
       " 1726              -1.192364          -0.715092          -0.054837   \n",
       " 1727              -0.808126          -0.083612           0.374733   \n",
       " 1728               1.214498           1.472177           0.658525   \n",
       " 1729               0.628051           0.217008           0.218133   \n",
       " 1730              -1.126717          -1.198443          -0.554713   \n",
       " 1731              -0.184921          -0.637386          -0.843822   \n",
       " 1732               1.081133           1.095438           0.535582   \n",
       " 1733              -0.734846          -0.752796          -0.752742   \n",
       " 1734              -0.328947          -0.446801          -0.877934   \n",
       " 1735              -0.392809           0.483204          -0.254984   \n",
       " 1736               0.941609           1.213558           0.537118   \n",
       " 1737              -1.186049           0.364250           1.494042   \n",
       " 1738               0.901400          -0.112365          -0.329182   \n",
       " 1739              -0.820821          -0.919315          -0.759387   \n",
       " 1740              -0.559748          -1.176600          -1.231483   \n",
       " 1741               0.134933          -0.634520          -0.545570   \n",
       " 1742               1.616158           1.112662           2.311500   \n",
       " 1743              -0.228945          -0.560010          -0.946097   \n",
       " 1744              -0.829377          -1.131539          -0.877848   \n",
       " 1745              -1.446064          -0.769989           0.859389   \n",
       " 1746              -0.065608          -0.303037          -0.791375   \n",
       " 1747              -0.405457          -0.794227          -1.037885   \n",
       " \n",
       "           Resp_Ampup_kurtosis  Resp_Ampup_skewness  \n",
       " eventidx                                            \n",
       " 18                   1.862772            -1.770874  \n",
       " 19                   0.495575            -0.819280  \n",
       " 20                  -0.326782             1.312886  \n",
       " 21                   0.110597            -0.272801  \n",
       " 22                   1.446658            -1.373992  \n",
       " 23                  -0.326782            -1.239706  \n",
       " 24                  -1.565973             0.298681  \n",
       " 25                   1.680301            -1.654637  \n",
       " 26                   0.963512             0.756298  \n",
       " 27                   0.623737             0.633040  \n",
       " 28                   1.413305            -1.469752  \n",
       " 29                   1.561352            -1.550393  \n",
       " 30                  -1.431176             0.105284  \n",
       " 31                  -1.789131             0.083254  \n",
       " 32                   0.336526             0.388743  \n",
       " 33                  -0.326782             0.432308  \n",
       " 34                  -0.326782             1.661079  \n",
       " 35                   0.830567             1.321410  \n",
       " 36                   0.266541            -0.935800  \n",
       " 37                   0.870340            -0.122808  \n",
       " 38                  -0.326782             1.131340  \n",
       " 39                   1.045232             0.925260  \n",
       " 40                  -0.326782             0.886734  \n",
       " 41                  -0.805222             0.408238  \n",
       " 42                  -1.867640             0.185199  \n",
       " 43                  -0.326782            -1.523181  \n",
       " 44                  -0.623139            -0.478699  \n",
       " 45                  -0.302457             0.138065  \n",
       " 46                   0.112556             0.164340  \n",
       " 47                   1.247748             1.280204  \n",
       " ...                       ...                  ...  \n",
       " 1718                -0.938226            -0.338891  \n",
       " 1719                 0.222810             0.405484  \n",
       " 1720                 0.839830             1.251098  \n",
       " 1721                 0.251787            -0.457852  \n",
       " 1722                -2.057145            -0.049205  \n",
       " 1723                 0.739055             1.091675  \n",
       " 1724                 0.848810             1.321364  \n",
       " 1725                 0.342312             1.026225  \n",
       " 1726                 0.805515            -1.163009  \n",
       " 1727                -0.170311             0.465794  \n",
       " 1728                 1.123174             1.414004  \n",
       " 1729                 0.577747            -1.242435  \n",
       " 1730                 1.383937            -1.602046  \n",
       " 1731                 0.692745            -1.024583  \n",
       " 1732                 0.863152             1.226902  \n",
       " 1733                -0.818670            -0.330232  \n",
       " 1734                -0.938226            -1.173209  \n",
       " 1735                 1.358956             1.560734  \n",
       " 1736                 1.176306             1.435948  \n",
       " 1737                -1.213911            -0.529235  \n",
       " 1738                -0.938226            -0.681562  \n",
       " 1739                 0.557299            -1.197265  \n",
       " 1740                -0.938226            -1.366821  \n",
       " 1741                 0.008445            -0.539122  \n",
       " 1742                -0.539328             0.646290  \n",
       " 1743                -0.938226             0.264633  \n",
       " 1744                 0.741026            -1.062270  \n",
       " 1745                -2.303544            -0.016269  \n",
       " 1746                -0.938226            -0.197864  \n",
       " 1747                 0.199356             0.861717  \n",
       " \n",
       " [1727 rows x 155 columns],\n",
       "           sujet  ECG_raw_mean  ECG_raw_median  ECG_raw_max  ECG_raw_min  \\\n",
       " eventidx                                                                  \n",
       " 18            2      0.216704        0.432240    -1.731048     0.539929   \n",
       " 19            2     -0.211167       -0.924700    -2.012164     0.703781   \n",
       " 20            2     -0.344355       -1.324807    -2.147606     0.629845   \n",
       " 21            2     -0.033802       -0.332407    -2.081662     0.728265   \n",
       " 22            2     -3.921855       -0.198297    -1.911559    -6.451396   \n",
       " 23            2     -0.244218       -0.560568    -1.479642     0.638363   \n",
       " 24            2      0.505654       -1.084018    -1.850297     0.719876   \n",
       " 25            2      0.087381       -1.803500    -1.976520     0.579884   \n",
       " 26            2     -0.269876       -0.821902    -1.609901     0.482189   \n",
       " 27            2      0.011095        0.321081     0.441905     0.089406   \n",
       " 28            2      0.128019       -0.246885     0.070824    -0.137981   \n",
       " 29            2      0.191153        0.304689     0.376451     0.029246   \n",
       " 30            2      0.758839       -0.010005     0.096468     0.185397   \n",
       " 31            2      0.171838       -0.769639     0.145954     0.185397   \n",
       " 32            2     -0.088033        0.493962     0.107501     0.107386   \n",
       " 33            2     -0.019996        0.523137     0.022477     0.232457   \n",
       " 34            2     -0.105199        0.720214    -0.060916     0.234100   \n",
       " 35            2     -0.179398       -1.026915    -0.484145     0.427551   \n",
       " 36            2     -0.908569       -1.119384    -0.228743     0.350093   \n",
       " 37            2      1.080465       -0.467683    -0.279840     0.314124   \n",
       " 38            2     -0.360955        0.617183    -0.223226     0.283175   \n",
       " 39            2     -0.185533       -1.022090    -0.008916     0.174218   \n",
       " 40            2      0.253340       -1.310899     0.119926     0.206341   \n",
       " 41            2     -0.212144       -1.299525     0.225809     0.073460   \n",
       " 42            2     -0.777423       -3.007306     0.327126     0.162567   \n",
       " 43            2      2.021698       -0.932333     0.827913    -0.165010   \n",
       " 44            2     -0.056593       -0.285425     0.461028    -0.068710   \n",
       " 45            2     -0.147570        1.667904     0.762616    -0.135505   \n",
       " 46            2      0.028898        1.678985     0.590553    -0.132714   \n",
       " 47            2     -0.057800        1.177435     0.532848    -0.037179   \n",
       " ...         ...           ...             ...          ...          ...   \n",
       " 1718         58      0.865458        1.585193    -0.574058     0.439612   \n",
       " 1719         58     -0.786369        2.006072    -0.791388     0.475938   \n",
       " 1720         58      0.017996       -0.869160     3.978006    -0.176992   \n",
       " 1721         58      0.960576        1.025056    -0.990113     0.517263   \n",
       " 1722         58     -1.082848        1.591977    -0.942494     0.117281   \n",
       " 1723         58     -0.000617        0.842103     0.923733     0.231129   \n",
       " 1724         58      0.522464        0.862716    -0.112128     0.431798   \n",
       " 1725         58     -0.032488       -0.800466    -0.708297     0.577614   \n",
       " 1726         58      0.551047        0.037761    -1.095465     0.620367   \n",
       " 1727         58     -1.704121       -0.041217    -0.809375    -1.681880   \n",
       " 1728         58     -2.065543        0.373737    -0.806564    -1.708470   \n",
       " 1729         58      1.385065        1.262903     0.012099     0.512274   \n",
       " 1730         58      0.499531        0.516575    -0.181794     0.449063   \n",
       " 1731         58      0.420874        0.692590    -0.560907     0.496652   \n",
       " 1732         58      0.819704        0.615089    -0.679033     0.633257   \n",
       " 1733         58      0.416111       -0.710857    -0.131085     0.538097   \n",
       " 1734         58     -1.731668        0.070184    -0.445147    -3.044875   \n",
       " 1735         58      0.321917       -0.068952     0.184897     0.485097   \n",
       " 1736         58      0.221309       -0.832163     0.004979     0.549700   \n",
       " 1737         58     -1.971405       -0.518783     0.070497    -1.569968   \n",
       " 1738         58      0.414549        0.561503    -0.105444     0.480090   \n",
       " 1739         58      0.427527        0.092948     0.487519     0.416303   \n",
       " 1740         58      0.273019       -0.293565    -0.134537     0.426546   \n",
       " 1741         58      0.757898       -0.670748    -0.402049     0.386852   \n",
       " 1742         58      0.450048       -1.107744     1.930359     0.165124   \n",
       " 1743         58      0.244684       -1.244060     0.956931     0.340856   \n",
       " 1744         58      1.112906       -1.556704     0.410290     0.410958   \n",
       " 1745         58     -2.158211       -2.168683     0.379375    -2.517830   \n",
       " 1746         58      0.560952       -0.893299     0.172760     0.472390   \n",
       " 1747         58      0.289636       -0.360006    -0.041569     0.525753   \n",
       " \n",
       "           ECG_raw_vrange  ECG_raw_var  ECG_raw_stddev  ECG_raw_avgder  \\\n",
       " eventidx                                                                \n",
       " 18             -1.612692    -1.702185       -1.708843        0.297847   \n",
       " 19             -1.930263    -1.889691       -1.907570        0.243820   \n",
       " 20             -1.971521    -1.547217       -1.545924       -0.284609   \n",
       " 21             -1.997061    -2.457393       -2.520420        0.404007   \n",
       " 22              3.370585    -1.953534       -1.975642       -4.528600   \n",
       " 23             -1.507742    -2.082594       -2.113896        0.052828   \n",
       " 24             -1.828131    -1.323440       -1.312725       -0.695027   \n",
       " 25             -1.814620    -1.774013       -1.784761       -0.042905   \n",
       " 26             -1.485243    -1.482620       -1.478361       -0.341006   \n",
       " 27              0.245590     0.335389        0.346591        0.597842   \n",
       " 28              0.150692     0.512687        0.517318       -2.327227   \n",
       " 29              0.243508     0.253577        0.267408        1.311100   \n",
       " 30             -0.067637     0.659402        0.657703       -1.722497   \n",
       " 31             -0.032817     0.381950        0.391542        0.297373   \n",
       " 32             -0.002852     0.070856        0.089621        0.218702   \n",
       " 33             -0.154098    -0.095960       -0.073841       -0.139113   \n",
       " 34             -0.213977     0.325612        0.337142        0.055671   \n",
       " 35             -0.653180     0.445296        0.452565        0.130077   \n",
       " 36             -0.416852     0.188518        0.204256       -0.484606   \n",
       " 37             -0.426514    -0.300426       -0.275728        2.612501   \n",
       " 38             -0.364057    -0.330185       -0.305255        0.069415   \n",
       " 39             -0.133618     0.310930        0.322945        0.701631   \n",
       " 40             -0.066440     0.848475        0.837452        0.030079   \n",
       " 41              0.105192     0.412225        0.420726        0.036714   \n",
       " 42              0.111350     0.801729        0.793132       -0.071341   \n",
       " 43              0.703165     1.917530        1.830326       -0.050963   \n",
       " 44              0.374621     0.534632        0.538367        0.136712   \n",
       " 45              0.635653    -0.336463       -0.311489        0.217280   \n",
       " 46              0.512542    -0.032019       -0.011055        0.680779   \n",
       " 47              0.402108     0.450710        0.457773        0.182209   \n",
       " ...                  ...          ...             ...             ...   \n",
       " 1718           -0.616247    -1.038260       -1.045854        0.226351   \n",
       " 1719           -0.727646    -1.655082       -1.696981       -0.284658   \n",
       " 1720            1.581905     1.940854        1.904566        0.205794   \n",
       " 1721           -0.837107    -0.456733       -0.445722       -0.525185   \n",
       " 1722           -0.445391    -1.733870       -1.781285       -0.578048   \n",
       " 1723            0.112269    -0.220796       -0.205841       -0.830323   \n",
       " 1724           -0.444489    -1.102003       -1.112430        0.295663   \n",
       " 1725           -0.793334     0.041077        0.058069       -0.453819   \n",
       " 1726           -0.971214    -0.170281       -0.154742        0.251316   \n",
       " 1727            1.287726    -0.437701       -0.426297       -2.030323   \n",
       " 1728            1.313640    -0.958011       -0.962266        2.883019   \n",
       " 1729           -0.475670    -0.837547       -0.837260       -0.084072   \n",
       " 1730           -0.485465    -0.858287       -0.858742       -0.096406   \n",
       " 1731           -0.665009    -0.964288       -0.968795       -0.021223   \n",
       " 1732           -0.835051    -1.075209       -1.084426        0.150875   \n",
       " 1733           -0.550835    -0.133117       -0.117208       -0.082016   \n",
       " 1734            2.694445     0.896661        0.903914        1.693011   \n",
       " 1735           -0.388694    -0.072435       -0.056027        0.161448   \n",
       " 1736           -0.513271     0.291987        0.308678       -0.183337   \n",
       " 1737            1.496082     0.615982        0.629118        2.258941   \n",
       " 1738           -0.487357     0.651251        0.663788       -0.004777   \n",
       " 1739           -0.216511     0.283063        0.299802        0.106236   \n",
       " 1740           -0.447546     0.509882        0.524569       -0.498460   \n",
       " 1741           -0.505582     0.299094        0.315746       -0.057346   \n",
       " 1742            0.532447     0.597620        0.611051        0.490373   \n",
       " 1743            0.021278     1.631754        1.611796       -0.092589   \n",
       " 1744           -0.238996     1.574302        1.557067        0.280977   \n",
       " 1745            2.494138     2.142651        2.094199       -2.320772   \n",
       " 1746           -0.381109     0.044799        0.061802       -0.034733   \n",
       " 1747           -0.507404     0.192641        0.209711       -0.825917   \n",
       " \n",
       "           ECG_raw_maxgra         ...           Resp_RR_skewness  \\\n",
       " eventidx                         ...                              \n",
       " 18             -1.838276         ...                   0.371954   \n",
       " 19             -1.854636         ...                  -1.723504   \n",
       " 20             -1.974902         ...                  -1.174364   \n",
       " 21             -1.932792         ...                   0.801668   \n",
       " 22             -1.610302         ...                   1.018976   \n",
       " 23             -1.380595         ...                   0.374757   \n",
       " 24             -1.553065         ...                   0.880621   \n",
       " 25             -1.861799         ...                  -0.982274   \n",
       " 26             -1.471723         ...                  -0.354698   \n",
       " 27              0.283704         ...                   0.490788   \n",
       " 28              2.895557         ...                  -0.504279   \n",
       " 29              0.349064         ...                   1.381745   \n",
       " 30              1.729955         ...                  -0.850334   \n",
       " 31              0.168322         ...                   1.131995   \n",
       " 32              0.011770         ...                   0.749449   \n",
       " 33             -0.130596         ...                  -1.309500   \n",
       " 34             -0.103779         ...                  -1.360140   \n",
       " 35             -0.356856         ...                  -0.872559   \n",
       " 36             -0.283223         ...                   1.983568   \n",
       " 37             -0.168264         ...                   0.440315   \n",
       " 38             -0.320633         ...                   0.317385   \n",
       " 39             -0.158224         ...                   0.686736   \n",
       " 40              0.184347         ...                  -0.364859   \n",
       " 41              0.404417         ...                  -0.444779   \n",
       " 42              0.523461         ...                  -0.164530   \n",
       " 43              0.908271         ...                  -0.543678   \n",
       " 44              0.165941         ...                  -1.042548   \n",
       " 45              0.211999         ...                  -1.474739   \n",
       " 46              0.035976         ...                  -0.022041   \n",
       " 47              0.136767         ...                   1.274992   \n",
       " ...                  ...         ...                        ...   \n",
       " 1718           -0.893040         ...                   0.901153   \n",
       " 1719           -1.141142         ...                   0.461693   \n",
       " 1720            2.949657         ...                  -1.215336   \n",
       " 1721           -0.950570         ...                   1.226142   \n",
       " 1722           -1.358306         ...                   0.206569   \n",
       " 1723            0.193525         ...                  -0.630922   \n",
       " 1724           -0.798467         ...                  -1.102935   \n",
       " 1725            2.934778         ...                  -1.225257   \n",
       " 1726           -0.754359         ...                  -1.495213   \n",
       " 1727           -0.697946         ...                   1.897534   \n",
       " 1728           -0.935004         ...                  -0.169218   \n",
       " 1729           -0.362642         ...                  -0.378579   \n",
       " 1730           -0.606771         ...                   0.318207   \n",
       " 1731           -0.606771         ...                  -0.239983   \n",
       " 1732           -0.482100         ...                   1.552172   \n",
       " 1733            0.005561         ...                  -1.240028   \n",
       " 1734           -0.092887         ...                   1.641630   \n",
       " 1735            0.318607         ...                   0.123014   \n",
       " 1736           -0.028079         ...                  -0.237227   \n",
       " 1737            0.055603         ...                  -1.176490   \n",
       " 1738            0.099798         ...                   0.771931   \n",
       " 1739            0.414674         ...                  -0.191570   \n",
       " 1740           -0.354152         ...                  -1.108170   \n",
       " 1741           -0.387091         ...                   0.116299   \n",
       " 1742            0.961371         ...                  -1.024236   \n",
       " 1743            0.719177         ...                   1.791017   \n",
       " 1744            0.499732         ...                   0.569634   \n",
       " 1745            0.878053         ...                  -0.208512   \n",
       " 1746            0.285308         ...                  -0.698170   \n",
       " 1747            0.133484         ...                   0.764852   \n",
       " \n",
       "           Resp_Ampup_mean  Resp_Ampup_max  Resp_Ampup_min  Resp_Ampup_vrange  \\\n",
       " eventidx                                                                       \n",
       " 18              -0.396525       -0.702818        0.286562          -1.103337   \n",
       " 19              -0.544194       -0.613709       -0.313486          -0.626660   \n",
       " 20              -0.336137       -0.215080       -0.337413          -0.083672   \n",
       " 21              -0.978174       -1.016277       -0.977802          -0.763689   \n",
       " 22              -0.954119       -1.001402       -0.607926          -0.964939   \n",
       " 23              -0.892704       -0.914630       -0.737342          -0.772536   \n",
       " 24              -0.937583       -0.978581       -0.662886          -0.901837   \n",
       " 25              -0.726262       -0.861514       -0.372226          -0.920226   \n",
       " 26              -0.769855       -0.766360       -0.610437          -0.651708   \n",
       " 27              -0.403392       -0.189055       -1.004458           0.349363   \n",
       " 28              -0.707541       -0.842964       -0.556956          -0.785258   \n",
       " 29              -0.718825       -0.851007       -0.325429          -0.934249   \n",
       " 30              -0.783299       -0.842995       -0.950276          -0.550314   \n",
       " 31              -0.873128       -0.891080       -0.765755          -0.724328   \n",
       " 32              -0.494458       -0.613521       -0.673836          -0.411123   \n",
       " 33              -0.366627       -0.614882        0.029287          -0.833003   \n",
       " 34              -0.197630        0.108089       -0.565863           0.481426   \n",
       " 35               0.042905        0.161761        0.258364           0.060183   \n",
       " 36              -0.385991       -0.365981       -0.550054          -0.156769   \n",
       " 37              -0.835096       -0.724600       -0.706911          -0.538685   \n",
       " 38              -0.887846       -0.991841       -0.523038          -1.002974   \n",
       " 39              -0.765255       -0.802997       -0.360200          -0.849800   \n",
       " 40              -0.519607       -0.485540       -0.454766          -0.372267   \n",
       " 41              -0.416663       -0.328964       -0.454176          -0.164955   \n",
       " 42              -0.723302       -0.679974       -0.703773          -0.481373   \n",
       " 43              -0.687441       -0.702426       -0.660461          -0.537027   \n",
       " 44               1.078710        1.150288        0.316985           1.336223   \n",
       " 45               1.161032        1.321204        0.543482           1.427587   \n",
       " 46               0.388997        0.493275        0.693582           0.239847   \n",
       " 47              -0.056850       -0.330562        0.388047          -0.670252   \n",
       " ...                   ...             ...             ...                ...   \n",
       " 1718            -0.086896       -0.344639       -0.028141          -0.416718   \n",
       " 1719            -0.216921       -0.458942        0.233083          -0.758966   \n",
       " 1720             0.186737        0.038714        0.274445          -0.157776   \n",
       " 1721            -0.635798       -0.584790       -1.412098           0.321816   \n",
       " 1722             0.296125        0.258867        0.178494           0.194338   \n",
       " 1723             0.841044        2.342515        0.066091           2.926845   \n",
       " 1724             0.256940        0.127298        0.530397          -0.238233   \n",
       " 1725             0.367520        0.328038       -0.528919           0.815725   \n",
       " 1726             0.373708        0.047201        0.737378          -0.496107   \n",
       " 1727             0.484743        0.690336        0.223035           0.709025   \n",
       " 1728            -0.086445        0.634789       -1.020172           1.575995   \n",
       " 1729             0.296255       -0.216752        0.666840          -0.778323   \n",
       " 1730             0.851462        0.054853        1.501297          -1.062487   \n",
       " 1731             0.664833       -0.089060        1.044905          -0.901177   \n",
       " 1732             0.719643        0.608595        0.865397           0.120723   \n",
       " 1733             0.250777       -0.128861        0.325484          -0.409208   \n",
       " 1734             0.987562        2.348873        0.403964           2.680121   \n",
       " 1735             0.278930        0.332334        0.442018           0.088960   \n",
       " 1736            -0.274270        0.114380       -0.491477           0.515989   \n",
       " 1737             2.785106        1.961166        2.716615           0.443386   \n",
       " 1738             0.804892        0.538996        0.447321           0.347572   \n",
       " 1739            -0.233565       -0.519533       -0.077594          -0.601665   \n",
       " 1740            -0.070700       -0.445380        0.405359          -0.871653   \n",
       " 1741             0.363542        0.022520        0.477510          -0.331494   \n",
       " 1742            -2.566029       -2.272319       -1.882502          -1.467815   \n",
       " 1743            -1.252091       -1.138793       -0.957617          -0.724911   \n",
       " 1744            -1.140476       -1.179461       -1.002665          -0.742617   \n",
       " 1745            -1.639756       -0.908407       -1.783252           0.190491   \n",
       " 1746            -1.184685       -0.906415       -1.160877          -0.276337   \n",
       " 1747            -1.422189       -1.256122       -1.194319          -0.695497   \n",
       " \n",
       "           Resp_Ampup_avgder  Resp_Ampup_maxgra  Resp_Ampup_absdev  \\\n",
       " eventidx                                                            \n",
       " 18                -0.186527          -0.808399          -1.018827   \n",
       " 19                -0.661417          -0.989586          -0.619532   \n",
       " 20                 0.750637          -0.236101          -0.176494   \n",
       " 21                 0.072481          -0.375750          -0.780937   \n",
       " 22                -0.242849          -0.681193          -0.923214   \n",
       " 23                -0.100767          -0.476236          -0.724332   \n",
       " 24                -0.213376          -0.655083          -0.782532   \n",
       " 25                -0.254497          -0.703664          -0.852205   \n",
       " 26                -0.163685          -0.181955          -0.727857   \n",
       " 27                 0.263296           0.357189           0.287819   \n",
       " 28                 0.027608          -0.393269          -0.786084   \n",
       " 29                -0.318930          -0.794289          -0.847414   \n",
       " 30                -0.505681          -0.630582          -0.571731   \n",
       " 31                 0.106953          -0.537514          -0.635041   \n",
       " 32                 0.456544          -0.209445          -0.447255   \n",
       " 33                -0.476095          -0.794010          -0.794182   \n",
       " 34                 0.752382           0.875209           0.369028   \n",
       " 35                 0.315587           0.498189          -0.026860   \n",
       " 36                -0.906597          -0.942561           0.039981   \n",
       " 37                -0.083872          -0.087003          -0.476734   \n",
       " 38                -0.198756          -0.723234          -0.904716   \n",
       " 39                -0.447987          -0.837485          -0.826803   \n",
       " 40                 0.721665          -0.348673          -0.482687   \n",
       " 41                 0.313684          -0.097421          -0.103915   \n",
       " 42                -0.052192          -0.252142          -0.404772   \n",
       " 43                 0.014462          -0.023710          -0.462868   \n",
       " 44                 0.265925           0.985033           1.630050   \n",
       " 45                -2.506396          -1.589194           1.335978   \n",
       " 46                -0.357059           0.805780           0.113365   \n",
       " 47                -0.445256          -0.407339          -0.565451   \n",
       " ...                     ...                ...                ...   \n",
       " 1718              -1.134026          -0.641013          -0.643761   \n",
       " 1719               0.084911          -0.371023          -0.540416   \n",
       " 1720              -0.113480           0.018221          -0.247292   \n",
       " 1721              -0.488016           0.971355           0.238067   \n",
       " 1722               0.602291           0.033746           1.173866   \n",
       " 1723               3.148546           2.772206           2.173197   \n",
       " 1724               0.238512           0.313321          -0.033548   \n",
       " 1725              -1.314807          -0.357421           0.530611   \n",
       " 1726              -0.514156          -0.662158          -0.453445   \n",
       " 1727              -0.843277          -0.039387           0.669346   \n",
       " 1728               1.235154           1.482027           2.461087   \n",
       " 1729              -0.764460          -0.779947          -0.903182   \n",
       " 1730              -0.304613          -0.960861          -1.148895   \n",
       " 1731              -0.090846          -0.634409          -1.085836   \n",
       " 1732              -0.601264           0.735080          -0.318114   \n",
       " 1733               0.147822          -0.362536          -0.605752   \n",
       " 1734               2.943576           2.559458           1.983290   \n",
       " 1735               0.322168           0.539203           0.262167   \n",
       " 1736              -0.370045           0.873966           0.341297   \n",
       " 1737               0.119188           0.496224           0.800571   \n",
       " 1738               0.477309          -0.368626           0.210042   \n",
       " 1739               0.217170          -0.594149          -0.677142   \n",
       " 1740              -0.997673          -1.203565          -0.967626   \n",
       " 1741               0.252815          -0.672481          -0.127592   \n",
       " 1742              -0.502400          -1.131385          -1.539481   \n",
       " 1743              -0.595053          -0.705717          -0.752415   \n",
       " 1744              -0.918684          -1.015249          -0.805255   \n",
       " 1745              -0.065814           0.809398           0.969205   \n",
       " 1746              -0.310064          -0.413907          -0.261920   \n",
       " 1747               0.139217          -0.690370          -0.701074   \n",
       " \n",
       "           Resp_Ampup_kurtosis  Resp_Ampup_skewness  \n",
       " eventidx                                            \n",
       " 18                  -0.819272             0.230219  \n",
       " 19                  -0.101284             0.160726  \n",
       " 20                   0.167693             0.794548  \n",
       " 21                   1.687419            -1.984913  \n",
       " 22                   1.251764             1.502533  \n",
       " 23                  -0.460769            -0.339928  \n",
       " 24                  -0.821801             0.371920  \n",
       " 25                  -0.636348            -0.564539  \n",
       " 26                   0.873328             0.477630  \n",
       " 27                   0.335048            -0.335528  \n",
       " 28                   1.568625            -1.934767  \n",
       " 29                  -0.991672             0.003790  \n",
       " 30                   1.906141            -2.076469  \n",
       " 31                  -1.145863            -0.555288  \n",
       " 32                   1.112672            -1.688588  \n",
       " 33                   0.538038            -1.354720  \n",
       " 34                  -0.309639             0.615335  \n",
       " 35                   0.526074             1.259570  \n",
       " 36                  -0.507753            -0.381308  \n",
       " 37                   0.583398             1.002790  \n",
       " 38                  -1.353695            -0.449050  \n",
       " 39                   1.471579             1.780143  \n",
       " 40                   0.196841             0.476637  \n",
       " 41                  -0.821527             0.018029  \n",
       " 42                  -1.051210             0.202108  \n",
       " 43                  -0.880856            -0.458419  \n",
       " 44                  -1.217520             0.036023  \n",
       " 45                  -0.601941             0.087329  \n",
       " 46                   0.598115             1.340739  \n",
       " 47                  -0.222318            -0.469088  \n",
       " ...                       ...                  ...  \n",
       " 1718                 0.762935            -1.010343  \n",
       " 1719                -1.558839             0.486189  \n",
       " 1720                -0.132967             0.423079  \n",
       " 1721                 0.792578            -1.555206  \n",
       " 1722                -1.215945             0.079919  \n",
       " 1723                 1.785696             1.885578  \n",
       " 1724                -1.112262             0.654532  \n",
       " 1725                 1.130360            -1.243776  \n",
       " 1726                -0.437314             0.415880  \n",
       " 1727                 0.571435             0.979985  \n",
       " 1728                -0.868308             0.653298  \n",
       " 1729                 0.327405            -0.761743  \n",
       " 1730                -0.153183            -0.033127  \n",
       " 1731                 1.821479            -2.137822  \n",
       " 1732                 1.514602             1.553802  \n",
       " 1733                 0.630156            -0.992689  \n",
       " 1734                 1.803192             1.920737  \n",
       " 1735                -0.638502             0.839728  \n",
       " 1736                 0.430737             0.863279  \n",
       " 1737                -1.081164             0.188873  \n",
       " 1738                -0.189302            -0.483139  \n",
       " 1739                 0.030761            -0.880571  \n",
       " 1740                -0.025927             0.083345  \n",
       " 1741                -0.578997            -0.356026  \n",
       " 1742                -1.416495            -0.121651  \n",
       " 1743                -0.778304            -0.183835  \n",
       " 1744                 0.930277            -1.697020  \n",
       " 1745                -0.921177             0.489439  \n",
       " 1746                -0.602313             0.035981  \n",
       " 1747                -0.820617            -0.096696  \n",
       " \n",
       " [1728 rows x 154 columns],\n",
       "           sujet  ECG_raw_mean  ECG_raw_median  ECG_raw_max  ECG_raw_min  \\\n",
       " eventidx                                                                  \n",
       " 18            2     -0.676255        0.017119    -1.789465     0.489156   \n",
       " 19            2     -0.094325       -0.983920    -2.077616     0.630765   \n",
       " 20            2      0.071147       -0.768972    -2.216446     0.570811   \n",
       " 21            2      0.076285       -0.311708    -2.147545     0.644436   \n",
       " 22            2     -0.085260       -0.732947    -1.974493     0.588749   \n",
       " 23            2      2.628999       -1.052035    -1.079746     0.578546   \n",
       " 24            2      0.065943       -0.997301    -1.911698     0.652571   \n",
       " 25            2      0.408588       -1.055385    -2.041079     0.525441   \n",
       " 26            2      0.053017       -0.996366    -1.665286     0.566664   \n",
       " 27            2      0.407040        0.773285     0.437862     0.075454   \n",
       " 28            2      0.125346        0.367130     0.370770     0.025391   \n",
       " 29            2      0.092698       -0.181852     0.370770     0.025391   \n",
       " 30            2      0.199305       -0.599536     0.246360     0.150090   \n",
       " 31            2     -0.307589       -1.073724     0.134506     0.167196   \n",
       " 32            2      0.008630        1.224600     0.095090     0.096352   \n",
       " 33            2     -0.030258        0.486698     0.063140     0.209932   \n",
       " 34            2      0.070758        0.896915    -0.077541     0.211424   \n",
       " 35            2      0.490948       -0.412985    -0.502929     0.350301   \n",
       " 36            2      0.021378       -0.537334    -0.070325     0.217254   \n",
       " 37            2      0.170572       -0.949383    -0.301943     0.284096   \n",
       " 38            2     -0.266776        0.265183    -0.243913     0.255991   \n",
       " 39            2     -1.775006       -1.358843    -0.024240     0.157043   \n",
       " 40            2     -4.752294       -1.047310     0.107827    -6.496654   \n",
       " 41            2      0.082873       -1.737531     0.216359     0.065542   \n",
       " 42            2      0.849747       -2.532165     0.320211     0.129005   \n",
       " 43            2     -0.155232       -1.313751     0.833529    -0.151019   \n",
       " 44            2     -0.418856       -0.196059     0.457464    -0.063566   \n",
       " 45            2     -0.168430        1.626464     0.766598    -0.124224   \n",
       " 46            2      0.099152        1.754416     0.766598    -0.121690   \n",
       " 47            2     -0.002055        1.211844     0.571552    -0.039328   \n",
       " ...         ...           ...             ...          ...          ...   \n",
       " 1718         58     -0.240209        1.375771    -0.388153     0.283128   \n",
       " 1719         58      0.548418        1.979735    -0.461838     0.314081   \n",
       " 1720         58      0.881404       -0.144614     2.740699    -0.379267   \n",
       " 1721         58      1.752172        0.911438     2.740699    -0.379267   \n",
       " 1722         58     -0.216019        1.491849     1.942349    -0.185237   \n",
       " 1723         58      0.556805        1.164508     0.374919     0.074078   \n",
       " 1724         58     -0.202324        0.974470    -0.427438     0.296984   \n",
       " 1725         58     -0.627441       -0.886610    -0.889219     0.458958   \n",
       " 1726         58      1.265804        0.382560    -1.189112     0.475015   \n",
       " 1727         58     -0.078155       -0.445259    -0.967511     0.374972   \n",
       " 1728         58     -0.427763       -0.070763    -0.884661     0.416316   \n",
       " 1729         58     -0.256206        1.150140    -0.597422     0.386377   \n",
       " 1730         58      1.371892        0.617010    -0.481400     0.316161   \n",
       " 1731         58     -0.338732        1.131895    -0.775054     0.369024   \n",
       " 1732         58     -0.603969        0.424497    -0.684639     0.468813   \n",
       " 1733         58      0.323103       -1.076799    -0.442121     0.415062   \n",
       " 1734         58      0.226969        0.315224     1.090378    -3.564946   \n",
       " 1735         58      0.235676        0.081048    -0.197368     0.356189   \n",
       " 1736         58     -1.403736       -1.082743    -0.336729     0.427950   \n",
       " 1737         58     -0.179177       -0.705006    -0.285980     0.331401   \n",
       " 1738         58      0.110861       -0.102223    -0.422261     0.350627   \n",
       " 1739         58      0.334597       -0.024224     0.037037     0.279771   \n",
       " 1740         58     -3.556692       -0.149536    -0.444796    -3.610195   \n",
       " 1741         58      0.132178       -0.313403    -0.584719     0.247058   \n",
       " 1742         58      0.118479       -1.138075     1.154632     0.000759   \n",
       " 1743         58      0.273162       -0.824529     0.400634     0.195964   \n",
       " 1744         58     -1.222974       -2.037319     0.252980     0.273835   \n",
       " 1745         58     -0.607159       -1.843874    -0.046729     0.298165   \n",
       " 1746         58      0.155181       -0.927702    -0.191333     0.342073   \n",
       " 1747         58      1.673856       -0.227469    -0.035844     0.366154   \n",
       " \n",
       "           ECG_raw_vrange  ECG_raw_var  ECG_raw_stddev  ECG_raw_avgder  \\\n",
       " eventidx                                                                \n",
       " 18             -1.369097    -1.972292       -1.992517        0.646479   \n",
       " 19             -1.632249    -1.873250       -1.886956       -0.115413   \n",
       " 20             -1.671956    -1.680142       -1.682600       -0.228325   \n",
       " 21             -1.682052    -2.011211       -2.034140       -0.085891   \n",
       " 22             -1.543958    -2.266504       -2.309191        0.191727   \n",
       " 23             -1.018623    -1.760440       -1.767342       -1.110380   \n",
       " 24             -1.550932    -1.780964       -1.789055        0.265792   \n",
       " 25             -1.539529    -1.836478       -1.847893        0.379222   \n",
       " 26             -1.349805    -1.557970       -1.554293        0.254398   \n",
       " 27              0.202432     0.187512        0.203296        1.057207   \n",
       " 28              0.197570     0.519208        0.522790        0.085548   \n",
       " 29              0.197570     0.471960        0.477540        0.145112   \n",
       " 30              0.040775     0.937582        0.919826       -1.114007   \n",
       " 31             -0.035653     0.482217        0.487370        0.003714   \n",
       " 32             -0.010362    -0.148370       -0.124669        0.165311   \n",
       " 33             -0.106034    -0.227187       -0.202296        0.431016   \n",
       " 34             -0.188556    -0.000534        0.020244        0.238342   \n",
       " 35             -0.529365    -0.088736       -0.066106        0.778556   \n",
       " 36             -0.188335    -0.083291       -0.060766        0.559465   \n",
       " 37             -0.367941    -0.008950        0.012019        0.099533   \n",
       " 38             -0.315226    -0.161338       -0.137424       -0.290996   \n",
       " 39             -0.120731     0.396579        0.405169       -0.326217   \n",
       " 40              4.475948     0.670907        0.667504       -5.383912   \n",
       " 41              0.080829     0.794195        0.784482       -1.055479   \n",
       " 42              0.097886     0.825313        0.813919       -0.008202   \n",
       " 43              0.585529     1.773964        1.694842       -0.099357   \n",
       " 44              0.308232     0.787758        0.778388       -0.681523   \n",
       " 45              0.528548    -0.211979       -0.187296        1.862612   \n",
       " 46              0.526826     0.230843        0.245277        0.317069   \n",
       " 47              0.357867     0.485286        0.490311       -0.007163   \n",
       " ...                  ...          ...             ...             ...   \n",
       " 1718           -0.389114    -1.384197       -1.401946        1.595557   \n",
       " 1719           -0.445048    -1.381781       -1.399419        0.679299   \n",
       " 1720            1.473129     1.547426        1.529849       -0.609096   \n",
       " 1721            1.473129     1.481992        1.467111       -1.084825   \n",
       " 1722            0.978285    -0.769268       -0.765062        0.440428   \n",
       " 1723            0.102426    -1.459290       -1.480594        0.881963   \n",
       " 1724           -0.416853    -0.970830       -0.972437        0.610906   \n",
       " 1725           -0.742214    -0.235601       -0.222286        0.455012   \n",
       " 1726           -0.883390    -0.399821       -0.388354       -0.177619   \n",
       " 1727           -0.709700    -0.375113       -0.363315        0.023032   \n",
       " 1728           -0.706713    -0.554935       -0.545988       -0.745378   \n",
       " 1729           -0.560040    -1.315830       -1.330511       -1.407680   \n",
       " 1730           -0.455070    -1.156607       -1.164758        0.882967   \n",
       " 1731           -0.622528    -1.322552       -1.337527       -0.145938   \n",
       " 1732           -0.662277    -0.811829       -0.808740        0.491221   \n",
       " 1733           -0.516039    -0.124939       -0.110846       -0.970168   \n",
       " 1734            3.271930     0.235786        0.249840        0.419308   \n",
       " 1735           -0.364814     0.164027        0.178399       -0.437613   \n",
       " 1736           -0.481004     0.471503        0.483450       -0.921387   \n",
       " 1737           -0.383295     0.139051        0.153497        0.471609   \n",
       " 1738           -0.456835     0.711652        0.719802        0.007444   \n",
       " 1739           -0.204222     0.629558        0.639191       -0.174602   \n",
       " 1740            2.649500     0.640205        0.649656       -3.558530   \n",
       " 1741           -0.444992    -0.239860       -0.226582       -0.329491   \n",
       " 1742            0.494318     1.708969        1.684256        0.697402   \n",
       " 1743            0.017560     1.217032        1.211899       -0.234948   \n",
       " 1744           -0.106991     1.236344        1.230564        1.546273   \n",
       " 1745           -0.254597     1.671800        1.648789        0.990586   \n",
       " 1746           -0.351122     0.582522        0.592919        0.264419   \n",
       " 1747           -0.303419     0.064585        0.079144        0.339850   \n",
       " \n",
       "           ECG_raw_maxgra         ...           Resp_one_Num  Resp_Ampup_mean  \\\n",
       " eventidx                         ...                                           \n",
       " 18             -2.047862         ...               0.491614        -0.449223   \n",
       " 19             -2.066838         ...               0.491614        -0.608117   \n",
       " 20             -2.206339         ...              -1.608920        -0.284586   \n",
       " 21             -2.030301         ...               0.491614        -0.940244   \n",
       " 22             -1.783428         ...               0.491614        -0.949861   \n",
       " 23             -1.516984         ...               0.491614        -0.954877   \n",
       " 24             -1.717037         ...               0.491614        -0.961725   \n",
       " 25             -2.075147         ...               0.491614        -0.689860   \n",
       " 26             -1.622686         ...              -1.608920        -0.831152   \n",
       " 27              0.413490         ...               0.491614        -0.397408   \n",
       " 28              0.489304         ...               0.491614        -0.678523   \n",
       " 29              0.489304         ...               0.491614        -0.753692   \n",
       " 30              0.228248         ...               0.491614        -0.710742   \n",
       " 31              0.279654         ...               0.491614        -0.769138   \n",
       " 32              0.098065         ...              -1.608920        -0.461349   \n",
       " 33              0.187868         ...              -1.608920        -0.296577   \n",
       " 34             -0.035964         ...              -1.608920        -0.134102   \n",
       " 35             -0.329516         ...               0.491614        -0.073928   \n",
       " 36             -0.174902         ...               2.592149        -0.242098   \n",
       " 37             -0.110762         ...               0.491614        -0.841418   \n",
       " 38             -0.287500         ...               0.491614        -0.928166   \n",
       " 39             -0.099116         ...               0.491614        -0.823631   \n",
       " 40              0.298242         ...              -1.608920        -0.544440   \n",
       " 41              0.553509         ...               0.491614        -0.428920   \n",
       " 42              0.691592         ...               0.491614        -0.750073   \n",
       " 43              1.137945         ...               0.491614        -0.650143   \n",
       " 44              0.276893         ...               0.491614         1.072732   \n",
       " 45              0.330317         ...               0.491614         0.663514   \n",
       " 46              0.330317         ...               0.491614         0.295495   \n",
       " 47              0.272284         ...               0.491614         0.150025   \n",
       " ...                  ...         ...                    ...              ...   \n",
       " 1718           -0.619658         ...              -1.554563        -0.130067   \n",
       " 1719           -1.058621         ...               0.000000        -0.365855   \n",
       " 1720            2.172068         ...              -1.554563        -0.053976   \n",
       " 1721            2.172068         ...              -1.554563        -0.301984   \n",
       " 1722            1.279996         ...               1.554563         0.641480   \n",
       " 1723           -0.211539         ...               0.000000         0.587787   \n",
       " 1724           -1.069452         ...               0.000000         0.068036   \n",
       " 1725            2.159200         ...               1.554563         0.298301   \n",
       " 1726           -0.941085         ...               0.000000         0.044781   \n",
       " 1727           -0.982517         ...               0.000000         0.464548   \n",
       " 1728           -1.010622         ...               0.000000        -0.188703   \n",
       " 1729           -0.692534         ...               0.000000         0.070731   \n",
       " 1730           -0.903665         ...               0.000000         1.416710   \n",
       " 1731           -0.903665         ...               0.000000         0.844023   \n",
       " 1732           -0.634446         ...               0.000000         0.958487   \n",
       " 1733           -0.374098         ...              -1.554563         0.282556   \n",
       " 1734            1.869598         ...              -1.554563         1.373710   \n",
       " 1735           -0.103363         ...               0.000000         0.230309   \n",
       " 1736           -0.304397         ...               0.000000        -0.365317   \n",
       " 1737           -0.330820         ...               0.000000         2.126722   \n",
       " 1738           -0.292597         ...              -1.554563         1.145253   \n",
       " 1739           -0.020281         ...               0.000000        -0.347859   \n",
       " 1740           -0.312575         ...               0.000000        -0.117612   \n",
       " 1741           -0.713678         ...               1.554563         0.480390   \n",
       " 1742            0.452523         ...               1.554563        -0.423636   \n",
       " 1743            0.243065         ...               0.000000        -1.738866   \n",
       " 1744            0.199447         ...               1.554563        -1.386136   \n",
       " 1745            0.380466         ...               1.554563        -1.996161   \n",
       " 1746            0.419476         ...               0.000000        -1.836518   \n",
       " 1747            0.131704         ...               0.000000        -1.781131   \n",
       " \n",
       "           Resp_Ampup_max  Resp_Ampup_min  Resp_Ampup_vrange  \\\n",
       " eventidx                                                      \n",
       " 18             -0.754732       -0.103999          -0.938180   \n",
       " 19             -0.718555       -0.559278          -0.644232   \n",
       " 20             -0.264912       -0.205224          -0.238035   \n",
       " 21             -1.060074       -0.638381          -1.051429   \n",
       " 22             -1.049494       -0.581031          -1.068513   \n",
       " 23             -0.974591       -0.842536          -0.828360   \n",
       " 24             -1.010085       -0.818842          -0.887943   \n",
       " 25             -0.860849       -0.338249          -0.951284   \n",
       " 26             -0.793360       -0.558323          -0.743313   \n",
       " 27             -0.236957       -0.236037          -0.184532   \n",
       " 28             -0.875553       -0.393132          -0.940970   \n",
       " 29             -0.893430       -0.776256          -0.757275   \n",
       " 30             -0.812774       -0.830221          -0.621810   \n",
       " 31             -0.872893       -0.575546          -0.838789   \n",
       " 32             -0.651032       -0.727011          -0.464528   \n",
       " 33             -0.621073        0.282962          -0.971396   \n",
       " 34              0.047547        0.081202           0.018722   \n",
       " 35              0.127070        0.093765           0.116706   \n",
       " 36              0.061373       -0.379132           0.285955   \n",
       " 37             -0.765716       -0.705795          -0.627114   \n",
       " 38             -1.036231       -0.565978          -1.059179   \n",
       " 39             -0.853417       -0.404974          -0.905398   \n",
       " 40             -0.500386       -0.310493          -0.491351   \n",
       " 41             -0.421325       -0.426140          -0.324621   \n",
       " 42             -0.722178       -0.484738          -0.689328   \n",
       " 43             -0.668512       -0.718860          -0.491971   \n",
       " 44              1.024600        0.210742           1.236017   \n",
       " 45              1.065296        0.652697           1.050566   \n",
       " 46              0.402463        0.453328           0.285062   \n",
       " 47              0.055905        0.441155          -0.164980   \n",
       " ...                  ...             ...                ...   \n",
       " 1718           -0.629109        0.442209          -0.839944   \n",
       " 1719           -0.620089        0.463203          -0.842189   \n",
       " 1720            0.339878       -0.210352           0.438857   \n",
       " 1721           -0.014832       -1.260380           0.644456   \n",
       " 1722            0.856561        0.639105           0.494842   \n",
       " 1723            2.178869        0.064958           2.074570   \n",
       " 1724           -0.201454        0.859064          -0.643989   \n",
       " 1725            0.072829        0.770053          -0.332034   \n",
       " 1726           -0.296743        0.573131          -0.586742   \n",
       " 1727            0.482468        0.518701           0.195763   \n",
       " 1728            0.271156       -0.693387           0.624839   \n",
       " 1729           -0.491271       -0.040996          -0.453981   \n",
       " 1730            1.754880        0.981630           1.185118   \n",
       " 1731           -0.315142        1.390785          -1.031940   \n",
       " 1732            0.410871        1.356239          -0.311310   \n",
       " 1733           -0.407633        0.183457          -0.490367   \n",
       " 1734            1.773794        0.804618           1.295947   \n",
       " 1735            0.050780        0.661972          -0.296877   \n",
       " 1736            0.164295       -0.885957           0.622086   \n",
       " 1737            1.776023       -1.945034           2.735366   \n",
       " 1738            0.265648        1.375718          -0.462026   \n",
       " 1739           -0.869695        0.348921          -1.024000   \n",
       " 1740           -0.657699        0.840133          -1.075608   \n",
       " 1741           -0.066417        0.268492          -0.204616   \n",
       " 1742            1.079483       -1.199360           1.671545   \n",
       " 1743           -1.433449       -1.189938          -0.765181   \n",
       " 1744           -1.517895       -0.412581          -1.253230   \n",
       " 1745           -1.192319       -1.785641          -0.220457   \n",
       " 1746           -1.197123       -1.930458          -0.149410   \n",
       " 1747           -1.566663       -0.988306          -0.999488   \n",
       " \n",
       "           Resp_Ampup_avgder  Resp_Ampup_maxgra  Resp_Ampup_absdev  \\\n",
       " eventidx                                                            \n",
       " 18                -0.123218          -0.731820          -0.953438   \n",
       " 19                -0.127995          -0.624444          -0.706549   \n",
       " 20                 0.792475          -0.389146          -0.023452   \n",
       " 21                -0.266165          -0.874688          -0.940169   \n",
       " 22                -0.392518          -0.851201          -1.000337   \n",
       " 23                -0.631494          -0.708521          -0.764253   \n",
       " 24                -0.614729          -0.841184          -0.834404   \n",
       " 25                -0.674149          -0.931658          -0.785862   \n",
       " 26                -0.367749          -0.458423          -0.714792   \n",
       " 27                -0.247273           0.012176          -0.188558   \n",
       " 28                -0.545633          -0.929733          -0.875456   \n",
       " 29                 0.014883          -0.475286          -0.781942   \n",
       " 30                -0.266590          -0.311671          -0.719303   \n",
       " 31                 0.000288          -0.731552          -0.770094   \n",
       " 32                 0.503772          -0.487724          -0.489831   \n",
       " 33                -0.724314          -0.948938          -0.881785   \n",
       " 34                 0.080655           0.461970          -0.012521   \n",
       " 35                -0.664252           0.149648          -0.095811   \n",
       " 36                -1.739282          -0.663125           0.145424   \n",
       " 37                -0.190782          -0.360032          -0.647728   \n",
       " 38                -0.399708          -0.915976          -0.943289   \n",
       " 39                -0.411431          -0.654190          -0.917056   \n",
       " 40                -0.494459          -0.564935          -0.457471   \n",
       " 41                 0.102782          -0.413964          -0.291143   \n",
       " 42                -0.368649          -0.501170          -0.578368   \n",
       " 43                 0.248463          -0.154849          -0.192650   \n",
       " 44                 2.717217           1.419613           2.034694   \n",
       " 45                -0.504260           1.337628           1.330979   \n",
       " 46                 0.243362           0.452240           0.039901   \n",
       " 47                -1.568166          -0.651893          -0.225339   \n",
       " ...                     ...                ...                ...   \n",
       " 1718               0.101046          -0.767374          -1.057522   \n",
       " 1719               0.041433          -0.529048          -0.579765   \n",
       " 1720               1.570692           0.258672          -0.046389   \n",
       " 1721              -0.857035           0.936059           0.448239   \n",
       " 1722               1.345384           0.382744           0.569439   \n",
       " 1723              -0.592067           2.854054           1.264726   \n",
       " 1724              -0.408961           0.132417          -0.675667   \n",
       " 1725              -0.117891          -0.477087          -0.138585   \n",
       " 1726               0.021826          -0.841445          -0.494231   \n",
       " 1727              -0.630369          -0.174665          -0.080532   \n",
       " 1728               0.354300           1.342707           1.212913   \n",
       " 1729               0.248916          -0.407249          -0.566860   \n",
       " 1730              -2.539524          -1.327920           0.185301   \n",
       " 1731              -0.450644          -0.994915          -1.020712   \n",
       " 1732              -0.245900           0.670118          -0.675654   \n",
       " 1733               0.390630           0.076727          -0.823677   \n",
       " 1734               2.371980           2.347856           1.711145   \n",
       " 1735               0.343546           0.408645          -0.114139   \n",
       " 1736               1.447350           0.863182           1.339856   \n",
       " 1737              -2.336985           0.398279           2.998450   \n",
       " 1738               0.311149          -0.591742          -0.307724   \n",
       " 1739               0.161956          -0.997258          -0.914298   \n",
       " 1740              -0.155123          -0.644838          -1.097271   \n",
       " 1741               0.704102          -0.569346          -0.012958   \n",
       " 1742              -1.039998           0.883325           1.424314   \n",
       " 1743              -0.578457          -0.988844          -0.674422   \n",
       " 1744              -0.435344          -1.170386          -1.182983   \n",
       " 1745               0.576822           0.131067           0.392568   \n",
       " 1746               0.327555          -0.287137          -0.223468   \n",
       " 1747               0.069609          -0.916598          -0.860097   \n",
       " \n",
       "           Resp_Ampup_kurtosis  Resp_Ampup_skewness  \n",
       " eventidx                                            \n",
       " 18                   2.560772            -2.520112  \n",
       " 19                   0.614415            -0.995620  \n",
       " 20                  -0.962222             0.307639  \n",
       " 21                  -0.781403            -0.804197  \n",
       " 22                   0.059458             0.631071  \n",
       " 23                  -0.497928            -0.422994  \n",
       " 24                  -0.345416            -0.272382  \n",
       " 25                  -1.144184            -0.480434  \n",
       " 26                   0.950573             1.157224  \n",
       " 27                   0.228582             0.884024  \n",
       " 28                   0.368784            -1.394050  \n",
       " 29                   1.495695            -1.997389  \n",
       " 30                   1.372262            -1.810993  \n",
       " 31                  -0.783432            -0.737416  \n",
       " 32                   0.807909            -1.697000  \n",
       " 33                  -0.889822            -0.275559  \n",
       " 34                   0.082426             0.859726  \n",
       " 35                   0.911323             1.219023  \n",
       " 36                   0.771877             0.946646  \n",
       " 37                   0.614361             0.742653  \n",
       " 38                  -1.329687             0.097067  \n",
       " 39                   2.099330             1.784530  \n",
       " 40                  -0.006427             0.698184  \n",
       " 41                  -0.729603            -0.091061  \n",
       " 42                  -0.392954             0.787915  \n",
       " 43                  -1.135302            -0.611822  \n",
       " 44                  -1.401695            -0.294678  \n",
       " 45                  -0.394974             0.636497  \n",
       " 46                   0.494757             0.933818  \n",
       " 47                   0.264788             0.647924  \n",
       " ...                       ...                  ...  \n",
       " 1718                 0.653180            -0.704604  \n",
       " 1719                -1.081525             0.380659  \n",
       " 1720                 0.930955             0.906108  \n",
       " 1721                -0.439326            -0.618664  \n",
       " 1722                 0.578458             0.855760  \n",
       " 1723                 2.314034             1.785313  \n",
       " 1724                -0.286565             0.908810  \n",
       " 1725                -0.003839             0.598777  \n",
       " 1726                -0.261662             0.359393  \n",
       " 1727                 0.751674             0.954390  \n",
       " 1728                -0.932074             0.351247  \n",
       " 1729                 1.064211            -1.709945  \n",
       " 1730                 2.187229             1.611441  \n",
       " 1731                 0.002702            -1.587074  \n",
       " 1732                 1.340329             1.141353  \n",
       " 1733                 1.691417            -2.057878  \n",
       " 1734                -0.803833             0.729119  \n",
       " 1735                -0.738404             0.439091  \n",
       " 1736                -0.968743             0.182105  \n",
       " 1737                -0.281336            -1.377821  \n",
       " 1738                -1.026770            -0.216420  \n",
       " 1739                -0.990071            -0.793994  \n",
       " 1740                -0.080841             0.479603  \n",
       " 1741                -0.506387            -0.711785  \n",
       " 1742                 0.501534             0.960518  \n",
       " 1743                -0.617090            -0.724893  \n",
       " 1744                -0.497063            -1.112298  \n",
       " 1745                -1.229787            -0.064291  \n",
       " 1746                -0.006265            -0.689792  \n",
       " 1747                -1.264142            -0.274228  \n",
       " \n",
       " [1730 rows x 155 columns],\n",
       "           sujet  ECG_raw_mean  ECG_raw_median  ECG_raw_max  ECG_raw_min  \\\n",
       " eventidx                                                                  \n",
       " 18            2      0.554095        0.135039    -1.876159     0.711454   \n",
       " 19            2      1.210546       -1.112784    -1.569259     0.881554   \n",
       " 20            2      0.092430       -0.809351    -2.163694     0.825320   \n",
       " 21            2      1.132947       -0.241639    -2.236124     0.844733   \n",
       " 22            2      0.106185       -1.079432    -2.062161     0.850335   \n",
       " 23            2     -0.138138       -1.530529    -1.617106     0.836107   \n",
       " 24            2      0.028040       -0.781984    -1.876441     0.737261   \n",
       " 25            2      0.220265       -0.997845    -1.841512     0.762052   \n",
       " 26            2      1.291447       -1.287736    -1.751327     0.819538   \n",
       " 27            2      0.219072        0.627550     0.362888     0.134556   \n",
       " 28            2      0.004116        0.223060     0.295443     0.064743   \n",
       " 29            2     -1.467045       -0.187341     0.300931    -0.047036   \n",
       " 30            2      0.216944       -0.400767     0.300931    -0.047036   \n",
       " 31            2      0.078924       -0.174042     0.057936     0.262487   \n",
       " 32            2     -0.184623        1.149930     0.018313     0.163697   \n",
       " 33            2     -0.265641        0.363323     0.018313     0.163697   \n",
       " 34            2      4.158676        0.239104     1.009447     0.324163   \n",
       " 35            2      0.007478       -0.847629    -0.466708     0.471735   \n",
       " 36            2     -1.155507       -0.089061    -0.147972     0.332292   \n",
       " 37            2     -0.014474       -0.676545    -0.380809     0.425502   \n",
       " 38            2     -0.085720        0.232887    -0.181629     0.235293   \n",
       " 39            2      0.019454       -1.340762    -0.101645     0.235293   \n",
       " 40            2      0.005967       -1.091046     0.031116     0.289010   \n",
       " 41            2     -0.189228       -1.648576     0.140220     0.120734   \n",
       " 42            2     -0.134996       -2.153712     0.244618     0.209231   \n",
       " 43            2     -0.564689       -1.957001     0.760638    -0.181256   \n",
       " 44            2     -0.050409        0.579850     0.620818    -0.143892   \n",
       " 45            2     -0.181787        1.143533     0.693355    -0.143892   \n",
       " 46            2      0.073205        2.126410     0.693355    -0.143892   \n",
       " 47            2      0.217014        1.254300     0.602926    -0.084517   \n",
       " ...         ...           ...             ...          ...          ...   \n",
       " 1718         58      2.224969        1.058853    -0.443913     0.239880   \n",
       " 1719         58     -0.963757        1.923337    -0.443913     0.239880   \n",
       " 1720         58     -0.169358        0.074181     2.758954    -0.672537   \n",
       " 1721         58      2.823325        1.263938     2.758954    -0.672537   \n",
       " 1722         58      0.523859        1.458516     1.941719    -0.405270   \n",
       " 1723         58     -2.089084        1.553214     0.337211    -0.048077   \n",
       " 1724         58      0.545191        0.783512    -0.484126     0.258965   \n",
       " 1725         58     -0.366706       -0.859176    -0.956831     0.482077   \n",
       " 1726         58     -0.237276        0.269368    -1.036975     0.405943   \n",
       " 1727         58      1.320511       -0.317866    -1.036975     0.366390   \n",
       " 1728         58     -0.224808        0.010538    -0.952165     0.423340   \n",
       " 1729         58     -0.569567        0.739989    -0.658132     0.382101   \n",
       " 1730         58     -0.020692        0.675624    -0.539365     0.285382   \n",
       " 1731         58     -0.265492        0.815155    -0.539365     0.285382   \n",
       " 1732         58     -0.075566        0.311288    -0.747412     0.458801   \n",
       " 1733         58     -0.340714       -0.749637    -0.499157     0.421613   \n",
       " 1734         58     -0.210052        0.567873     1.069594    -5.060653   \n",
       " 1735         58     -0.074075        0.099449    -0.248614     0.340518   \n",
       " 1736         58     -0.437786       -1.043504    -0.391272     0.358392   \n",
       " 1737         58      0.016567       -0.458536    -0.339323     0.306374   \n",
       " 1738         58      0.709224        0.490471    -0.361994     0.118678   \n",
       " 1739         58      1.292394       -0.471251    -0.008664     0.235256   \n",
       " 1740         58     -0.212746       -0.463599     0.018532     0.195523   \n",
       " 1741         58     -0.321288       -0.223269    -0.603834     0.190195   \n",
       " 1742         58     -0.457838       -1.878935     1.135369    -0.149069   \n",
       " 1743         58     -0.213904       -1.018042     0.363534     0.119816   \n",
       " 1744         58      0.565271       -2.206741     0.212387     0.136770   \n",
       " 1745         58     -0.414157       -1.353643    -0.094412     0.260592   \n",
       " 1746         58     -0.284748       -0.566986    -0.126544     0.259411   \n",
       " 1747         58     -2.071693       -0.484125    -0.083270     0.236865   \n",
       " \n",
       "           ECG_raw_vrange  ECG_raw_var  ECG_raw_stddev  ECG_raw_avgder  \\\n",
       " eventidx                                                                \n",
       " 18             -1.590147    -1.749946       -1.749685       -0.171934   \n",
       " 19             -1.485450    -1.847361       -1.852144       -0.026697   \n",
       " 20             -1.836469    -1.750871       -1.750656       -0.396933   \n",
       " 21             -1.893488    -2.259839       -2.291096       -1.155260   \n",
       " 22             -1.784873    -2.253556       -2.284347       -0.223918   \n",
       " 23             -1.491505    -2.142029       -2.164865       -0.134236   \n",
       " 24             -1.604328    -1.742452       -1.741822       -0.097728   \n",
       " 25             -1.595359    -1.802046       -1.804427       -0.160823   \n",
       " 26             -1.568663    -1.562367       -1.553654       -1.052087   \n",
       " 27              0.159914     0.205685        0.219360        0.920518   \n",
       " 28              0.154500     0.389483        0.396578        0.102668   \n",
       " 29              0.218664     0.412340        0.418530        1.501469   \n",
       " 30              0.218664     0.616421        0.613700       -0.127887   \n",
       " 31             -0.105218     0.359183        0.367448        0.205445   \n",
       " 32             -0.077054    -0.204341       -0.180541        0.162589   \n",
       " 33             -0.077054     0.113564        0.130068       -0.426695   \n",
       " 34              0.472030     0.478648        0.482107       -1.264388   \n",
       " 35             -0.555469     0.211042        0.224543       -0.307251   \n",
       " 36             -0.275246    -0.218592       -0.194557        0.163381   \n",
       " 37             -0.475255    -0.282414       -0.257419       -0.141379   \n",
       " 38             -0.244224    -0.306534       -0.281218       -0.183839   \n",
       " 39             -0.192889     0.366359        0.374350       -0.111220   \n",
       " 40             -0.136820     0.673642        0.668157       -0.150903   \n",
       " 41              0.024497     0.438032        0.443183        0.149889   \n",
       " 42              0.043492     0.897214        0.879830       -0.442170   \n",
       " 43              0.586533     1.682133        1.609651       -0.340583   \n",
       " 44              0.476522     0.485000        0.488189       -0.269156   \n",
       " 45              0.523079     0.434108        0.439419       -0.301298   \n",
       " 46              0.523079     0.178154        0.192708        0.503854   \n",
       " 47              0.432827     0.313847        0.323800        0.905439   \n",
       " ...                  ...          ...             ...             ...   \n",
       " 1718           -0.378550    -0.632023       -0.628773        1.959590   \n",
       " 1719           -0.378550    -1.391754       -1.417137        0.539856   \n",
       " 1720            1.804762     0.877647        0.886907        0.029642   \n",
       " 1721            1.804762     0.804270        0.814707        1.438932   \n",
       " 1722            1.224602    -1.138998       -1.152836        0.405457   \n",
       " 1723            0.197736    -1.090940       -1.102815        0.207111   \n",
       " 1724           -0.411071    -0.703185       -0.701855       -1.445547   \n",
       " 1725           -0.792528    -0.397338       -0.388837       -0.128250   \n",
       " 1726           -0.780893    -0.974908       -0.982341        0.847815   \n",
       " 1727           -0.754409    -0.410752       -0.402508        0.045302   \n",
       " 1728           -0.750907    -0.896690       -0.901366       -0.395754   \n",
       " 1729           -0.578946    -0.814835       -0.816829       -0.997311   \n",
       " 1730           -0.455878    -1.201642       -1.218151       -0.211759   \n",
       " 1731           -0.455878    -1.172536       -1.187789       -1.668030   \n",
       " 1732           -0.674134    -0.906080       -0.911077       -0.166092   \n",
       " 1733           -0.527359    -0.473770       -0.466798       -0.234600   \n",
       " 1734            3.913695     0.267199        0.281797       -0.083230   \n",
       " 1735           -0.350060    -0.010216        0.003385       -0.134122   \n",
       " 1736           -0.432063     0.333740        0.348255       -0.963387   \n",
       " 1737           -0.371728     1.055169        1.060988        0.365655   \n",
       " 1738           -0.257178     0.775066        0.785932        0.097497   \n",
       " 1739           -0.161781     0.927877        0.936248       -2.193255   \n",
       " 1740           -0.121824     0.290726        0.305309       -0.327900   \n",
       " 1741           -0.423790    -0.286806       -0.276402        1.752761   \n",
       " 1742            0.657195     2.418659        2.371270       -0.207849   \n",
       " 1743            0.098238     1.024932        1.031397       -0.243081   \n",
       " 1744            0.012685     2.055459        2.026733       -0.853121   \n",
       " 1745           -0.220841     1.224200        1.225974        1.978506   \n",
       " 1746           -0.235825     0.167179        0.181669        1.214492   \n",
       " 1747           -0.199483     0.280351        0.294943       -0.629330   \n",
       " \n",
       "           ECG_raw_maxgra         ...           Resp_RR_skewness  \\\n",
       " eventidx                         ...                              \n",
       " 18             -1.724088         ...                  -0.043551   \n",
       " 19             -1.738448         ...                  -2.143869   \n",
       " 20             -1.844010         ...                  -1.335326   \n",
       " 21             -1.646557         ...                  -0.024492   \n",
       " 22             -1.523988         ...                  -0.249738   \n",
       " 23             -1.322366         ...                  -1.213804   \n",
       " 24             -1.472883         ...                   0.747301   \n",
       " 25             -1.314749         ...                  -0.807527   \n",
       " 26             -1.402352         ...                  -1.450239   \n",
       " 27              0.138447         ...                  -0.012327   \n",
       " 28              0.195816         ...                  -0.145451   \n",
       " 29              0.195816         ...                   0.044621   \n",
       " 30              0.153511         ...                   0.451233   \n",
       " 31              0.037172         ...                   0.518818   \n",
       " 32             -0.100239         ...                   0.887051   \n",
       " 33             -0.032284         ...                  -0.021869   \n",
       " 34             -0.019908         ...                  -1.538321   \n",
       " 35             -0.423795         ...                  -0.805583   \n",
       " 36             -0.306797         ...                   1.879810   \n",
       " 37             -0.258261         ...                  -1.160309   \n",
       " 38             -0.234924         ...                  -0.423372   \n",
       " 39             -0.183031         ...                  -0.608267   \n",
       " 40              0.051237         ...                  -1.964067   \n",
       " 41              2.728360         ...                  -0.378583   \n",
       " 42              0.348890         ...                   0.458356   \n",
       " 43              0.686651         ...                  -0.104112   \n",
       " 44              0.035082         ...                   1.095058   \n",
       " 45              0.075509         ...                   0.753395   \n",
       " 46              0.075509         ...                   0.224196   \n",
       " 47              0.477469         ...                   2.321437   \n",
       " ...                  ...         ...                        ...   \n",
       " 1718           -0.663035         ...                   0.327716   \n",
       " 1719           -1.047241         ...                  -0.089527   \n",
       " 1720            2.189667         ...                  -1.334782   \n",
       " 1721            2.189667         ...                  -0.393857   \n",
       " 1722            1.278110         ...                   1.280996   \n",
       " 1723           -0.246002         ...                  -1.613054   \n",
       " 1724           -0.798911         ...                  -0.139808   \n",
       " 1725            2.176518         ...                  -0.248767   \n",
       " 1726           -0.991483         ...                  -1.370243   \n",
       " 1727           -1.033820         ...                  -1.270942   \n",
       " 1728           -1.020314         ...                   0.244666   \n",
       " 1729           -0.447159         ...                   0.443057   \n",
       " 1730           -0.953246         ...                   0.290461   \n",
       " 1731           -0.953246         ...                  -0.425897   \n",
       " 1732           -0.655620         ...                   0.873257   \n",
       " 1733           -0.370615         ...                  -1.048123   \n",
       " 1734            1.880590         ...                  -0.859326   \n",
       " 1735           -0.135464         ...                   0.541270   \n",
       " 1736           -0.340888         ...                   0.465031   \n",
       " 1737           -0.367888         ...                   3.123019   \n",
       " 1738           -0.328831         ...                  -0.177048   \n",
       " 1739           -0.020568         ...                   0.319194   \n",
       " 1740           -0.268635         ...                  -0.726039   \n",
       " 1741           -0.759109         ...                  -0.108516   \n",
       " 1742            0.432564         ...                  -1.219933   \n",
       " 1743            0.218531         ...                   0.419375   \n",
       " 1744            0.173961         ...                   0.193738   \n",
       " 1745            0.358934         ...                   1.631293   \n",
       " 1746            0.398796         ...                   0.843134   \n",
       " 1747            0.104737         ...                   0.029655   \n",
       " \n",
       "           Resp_Ampup_mean  Resp_Ampup_max  Resp_Ampup_min  Resp_Ampup_vrange  \\\n",
       " eventidx                                                                       \n",
       " 18              -0.419137       -0.727652       -0.117199          -0.859030   \n",
       " 19              -0.635541       -0.713786       -0.641264          -0.591203   \n",
       " 20              -0.436280       -0.335063       -0.171106          -0.339591   \n",
       " 21              -0.903289       -0.988476       -0.563381          -0.973837   \n",
       " 22              -0.958701       -1.031884       -0.647288          -0.988335   \n",
       " 23              -0.910682       -0.960059       -0.743771          -0.851915   \n",
       " 24              -0.964650       -0.993810       -0.708344          -0.911284   \n",
       " 25              -0.725272       -0.858844       -0.471295          -0.854823   \n",
       " 26              -0.817021       -0.792030       -0.515392          -0.749734   \n",
       " 27              -0.411551       -0.273179       -0.192056          -0.251763   \n",
       " 28              -0.639837       -0.811340       -0.324607          -0.865171   \n",
       " 29              -0.722887       -0.871409       -0.879660          -0.675513   \n",
       " 30              -0.758828       -0.893239       -0.945009          -0.671741   \n",
       " 31              -0.694545       -0.852261       -0.591244          -0.789234   \n",
       " 32              -0.360419       -0.649301        0.000451          -0.816714   \n",
       " 33              -0.312469       -0.648079        0.163756          -0.893202   \n",
       " 34              -0.177672       -0.032964       -0.018818          -0.032461   \n",
       " 35              -0.090731        0.048038        0.248922          -0.058523   \n",
       " 36              -0.231496       -0.046120       -0.277379           0.074531   \n",
       " 37              -0.811860       -0.766977       -0.598965          -0.678301   \n",
       " 38              -0.903342       -1.012394       -0.561539          -1.004795   \n",
       " 39              -0.830499       -0.820354       -0.895081          -0.603943   \n",
       " 40              -0.478564       -0.527616       -0.270468          -0.534254   \n",
       " 41              -0.485421       -0.473098       -0.427714          -0.390567   \n",
       " 42              -0.691262       -0.285276       -1.123139           0.177882   \n",
       " 43              -0.678683       -0.658619       -0.757280          -0.466400   \n",
       " 44               0.857416        0.845553        0.078808           1.025633   \n",
       " 45               1.131209        0.919652        0.828584           0.760581   \n",
       " 46               0.550356        0.308074        0.900098          -0.042648   \n",
       " 47               0.181277       -0.113613        0.659233          -0.457839   \n",
       " ...                   ...             ...             ...                ...   \n",
       " 1718             0.027640       -0.492538       -0.082075          -0.467159   \n",
       " 1719            -0.145677       -0.557728        0.376579          -0.767305   \n",
       " 1720            -0.053372        0.320018       -0.011097           0.336228   \n",
       " 1721             0.134103        0.228195       -0.949867           0.717868   \n",
       " 1722             0.670419        1.017686       -0.902411           1.509366   \n",
       " 1723             0.444628        1.929861        0.349396           1.816303   \n",
       " 1724             0.231056       -0.144565        0.939363          -0.626142   \n",
       " 1725             0.460375        0.092723        0.821963          -0.321421   \n",
       " 1726             0.462338        0.260691        1.003072          -0.239828   \n",
       " 1727             0.380572        0.464609       -0.071981           0.516501   \n",
       " 1728            -0.290163        0.280995       -1.685809           1.145959   \n",
       " 1729             0.134275       -0.405090        0.488661          -0.666512   \n",
       " 1730             1.597523        1.500149        2.153774           0.456528   \n",
       " 1731             1.243884        0.633899        1.841701          -0.279953   \n",
       " 1732             1.084451        0.400356        1.456688          -0.325792   \n",
       " 1733             0.402284       -0.341921        0.290411          -0.500628   \n",
       " 1734             1.088077        1.608698        0.155493           1.582946   \n",
       " 1735             0.135127        0.079176        0.609818          -0.227736   \n",
       " 1736             0.166419        0.218869       -0.126398           0.290260   \n",
       " 1737             1.732350        1.578824       -1.655171           2.471135   \n",
       " 1738            -1.645903       -1.707740       -0.485959          -1.517524   \n",
       " 1739            -0.258394       -0.730953        0.436356          -0.976597   \n",
       " 1740            -0.067429       -0.551175        0.505393          -0.825919   \n",
       " 1741             0.382976       -0.072642        0.388712          -0.272345   \n",
       " 1742            -0.247654        1.014156       -0.535756           1.319614   \n",
       " 1743            -1.390503       -1.212843       -1.073775          -0.707908   \n",
       " 1744            -0.878679       -0.768395       -0.247452          -0.668191   \n",
       " 1745            -1.564454       -0.999667       -1.462984          -0.290132   \n",
       " 1746            -2.751139       -2.282316       -1.655375          -1.517524   \n",
       " 1747            -1.485130       -1.361334       -0.871272          -0.964093   \n",
       " \n",
       "           Resp_Ampup_avgder  Resp_Ampup_maxgra  Resp_Ampup_absdev  \\\n",
       " eventidx                                                            \n",
       " 18                 0.043432          -0.669039          -0.858183   \n",
       " 19                 0.242812          -0.782768          -0.644146   \n",
       " 20                -0.049586          -0.433109          -0.223494   \n",
       " 21                -0.147459          -0.929082          -0.922618   \n",
       " 22                -0.174779          -0.920884          -0.933651   \n",
       " 23                -0.264990          -0.760844          -0.805616   \n",
       " 24                -0.086071          -0.902934          -0.823355   \n",
       " 25                 0.015745          -0.678704          -0.728213   \n",
       " 26                -0.057048          -0.504839          -0.767947   \n",
       " 27                 0.166787          -0.005648          -0.364167   \n",
       " 28                -0.426847          -1.006863          -0.772076   \n",
       " 29                 0.237047          -0.418280          -0.722821   \n",
       " 30                -0.603653          -0.833034          -0.699531   \n",
       " 31                 0.179834          -0.823788          -0.730657   \n",
       " 32                 0.302223          -0.762533          -0.639450   \n",
       " 33                -0.339946          -0.933347          -0.852115   \n",
       " 34                 0.179779           0.481738          -0.195166   \n",
       " 35                -0.020883           0.185047          -0.257338   \n",
       " 36                -0.543223          -0.276056           0.247967   \n",
       " 37                -0.155404          -0.397522          -0.772512   \n",
       " 38                -0.201414          -0.982641          -0.896167   \n",
       " 39                -0.372895          -0.628811          -0.704905   \n",
       " 40                -0.529386          -0.604987          -0.473282   \n",
       " 41                 0.264399          -0.454812          -0.310387   \n",
       " 42                 1.312644           0.304984          -0.072826   \n",
       " 43                 0.182008          -0.338470          -0.356544   \n",
       " 44                 1.305660           1.512324           1.967788   \n",
       " 45                -1.815134           1.518145           1.404967   \n",
       " 46                 0.180902           0.480371          -0.047449   \n",
       " 47                -0.318933          -0.621226          -0.459638   \n",
       " ...                     ...                ...                ...   \n",
       " 1718              -1.120477          -0.828756          -0.833961   \n",
       " 1719              -0.797532          -0.573009          -0.559035   \n",
       " 1720               0.417984           0.228302          -0.181799   \n",
       " 1721               0.972285           0.781530           0.423927   \n",
       " 1722              -2.246029           0.419135           0.909012   \n",
       " 1723              -0.071254           2.707662           1.133602   \n",
       " 1724              -0.021595           0.125196          -0.485793   \n",
       " 1725              -0.049740          -0.503632          -0.408021   \n",
       " 1726               1.244465          -0.506138           0.012040   \n",
       " 1727              -1.215296          -0.157441           0.392738   \n",
       " 1728               1.507984           1.246647           1.320512   \n",
       " 1729               0.593669          -0.850637          -0.475214   \n",
       " 1730              -0.061059           0.342366          -0.137372   \n",
       " 1731              -0.723877          -0.739004          -0.502006   \n",
       " 1732              -0.473964           0.625243          -0.436490   \n",
       " 1733              -1.078695          -0.795366          -0.792955   \n",
       " 1734               2.911607           2.260708           1.934438   \n",
       " 1735              -0.150403           0.381461          -0.074472   \n",
       " 1736               0.274559           0.719897           0.755153   \n",
       " 1737               1.457094           1.307525           3.140913   \n",
       " 1738              -0.098688          -1.358851          -1.486881   \n",
       " 1739              -0.016369          -0.976752          -0.610648   \n",
       " 1740              -0.376609          -0.647585          -0.853919   \n",
       " 1741              -0.511480          -0.587790           0.289038   \n",
       " 1742              -1.269127           0.857275           0.990454   \n",
       " 1743               0.438096          -0.678906          -0.352464   \n",
       " 1744               0.567555          -0.619683          -0.732992   \n",
       " 1745               0.243964           0.083382          -0.112722   \n",
       " 1746              -0.098688          -1.358851          -1.486881   \n",
       " 1747              -0.248375          -0.903929          -0.778202   \n",
       " \n",
       "           Resp_Ampup_kurtosis  Resp_Ampup_skewness  \n",
       " eventidx                                            \n",
       " 18                   1.462437            -2.285900  \n",
       " 19                  -0.105281            -0.890970  \n",
       " 20                  -0.249772             0.799432  \n",
       " 21                  -0.196816             0.243045  \n",
       " 22                  -0.032000            -0.099575  \n",
       " 23                  -0.435925            -0.561741  \n",
       " 24                  -0.374446             0.361439  \n",
       " 25                  -0.925076            -0.665576  \n",
       " 26                   1.450468             1.314526  \n",
       " 27                   0.831755             1.156836  \n",
       " 28                  -0.613274            -0.936988  \n",
       " 29                   2.033230            -2.601756  \n",
       " 30                   0.814579            -2.291876  \n",
       " 31                  -0.265546            -1.405575  \n",
       " 32                  -1.237497            -0.814757  \n",
       " 33                  -0.232279            -0.855554  \n",
       " 34                   0.707934             1.027023  \n",
       " 35                   1.234874             1.381125  \n",
       " 36                  -0.279633             0.530552  \n",
       " 37                   1.752399             1.334568  \n",
       " 38                  -1.081146            -0.401848  \n",
       " 39                   0.204084            -0.225735  \n",
       " 40                  -0.510713             0.064121  \n",
       " 41                  -0.565940            -0.053049  \n",
       " 42                   0.556431             0.526975  \n",
       " 43                  -0.725473            -0.362222  \n",
       " 44                  -1.204227            -0.351864  \n",
       " 45                  -1.109826            -0.336523  \n",
       " 46                  -0.292571             0.256356  \n",
       " 47                  -0.217431             0.487102  \n",
       " ...                       ...                  ...  \n",
       " 1718                 1.822850            -2.344389  \n",
       " 1719                -0.830505            -0.318031  \n",
       " 1720                 1.411856             1.258195  \n",
       " 1721                -0.252996            -0.758373  \n",
       " 1722                 0.318846            -0.286795  \n",
       " 1723                 2.352551             1.881328  \n",
       " 1724                -0.172934             0.546796  \n",
       " 1725                 0.541901             0.595504  \n",
       " 1726                 0.066115             0.717508  \n",
       " 1727                 0.007437             0.063065  \n",
       " 1728                -0.361589            -0.110399  \n",
       " 1729                -0.575938            -0.703929  \n",
       " 1730                 1.676518             1.712543  \n",
       " 1731                 0.852769             1.156802  \n",
       " 1732                 0.030899             0.461413  \n",
       " 1733                 1.597681            -2.372033  \n",
       " 1734                -0.662926             0.460184  \n",
       " 1735                -0.223262             0.585778  \n",
       " 1736                -0.797182            -0.133493  \n",
       " 1737                -0.459003            -1.304951  \n",
       " 1738                -1.495245            -0.480401  \n",
       " 1739                -1.125395            -0.369046  \n",
       " 1740                -0.091771            -0.477096  \n",
       " 1741                -0.871315            -0.516871  \n",
       " 1742                 0.794721             1.122698  \n",
       " 1743                -0.836464            -0.603698  \n",
       " 1744                 0.286649             0.872682  \n",
       " 1745                -0.657225             0.089397  \n",
       " 1746                -1.495245            -0.480401  \n",
       " 1747                -0.851798            -0.263989  \n",
       " \n",
       " [1730 rows x 157 columns]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Norm['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std\n",
      "=============\n",
      "0\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "=============\n",
      "1\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "=============\n",
      "2\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "=============\n",
      "3\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "raw\n",
      "=============\n",
      "0\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "=============\n",
      "1\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "=============\n",
      "2\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "=============\n",
      "3\n",
      " valence\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      " arousal\n",
      "   DummyClassifier\n",
      "   Nearest Neighbors\n",
      "   Decision Tree\n",
      "   Random Forest\n",
      "range\n",
      "=============\n",
      "0\n",
      " valence\n",
      "   DummyClassifier\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0f4656de8923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'   '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mclfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mncmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Norm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnorname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'len'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlenidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mclfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mcmpclassifierdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmpclassifierdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv_iter)\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/dummy.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classes_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# numpy random_state expects Python int and not long as size argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# under Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/wenlu/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "Norm={'raw':df_rawn,'std':df_stdn,'range':df_rangen}\n",
    "'''\n",
    "DF={#\"phyEventdf\":phyEventdf, \n",
    "    \"phyEventNormdf\":phyEventNormdf, \n",
    "    #\"phyEventSubdf\":phyEventSubdf, \n",
    "    \"phyEventSubNormdf\":phyEventSubNormdf}\n",
    "'''\n",
    "names = [\"DummyClassifier\",\"Nearest Neighbors\", \n",
    "         #\"RBF SVM\", \n",
    "         \"Decision Tree\",\n",
    "         \"Random Forest\"]\n",
    "classifiers = [\n",
    "    DummyClassifier(strategy='most_frequent',random_state=0),\n",
    "    KNeighborsClassifier(3),\n",
    "    #SVC(gamma=0.0001, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=50)]\n",
    "\n",
    "cmpclassifierdf=pd.DataFrame()\n",
    "for norname,DFl in Norm.iteritems():\n",
    "    print norname\n",
    "    for lenidx,X in enumerate(DFl):\n",
    "        print(\"=============\")\n",
    "        print(lenidx)\n",
    "        Xn=X.dropna(axis=1,how='any')\n",
    "        y_all=eventLabel.ix[18:,:]\n",
    "        X_y=pd.concat([Xn,y_all],axis=1,join='inner')  \n",
    "        for label_i in ['valence','arousal']:\n",
    "            print(' '+label_i)\n",
    "            y=X_y[label_i]\n",
    "            Xn=X_y.iloc[:,:-9]\n",
    "            for clfname, clf in zip(names, classifiers):\n",
    "                print('   '+clfname)\n",
    "                scores = cross_val_score(clf, Xn, y, cv=10)\n",
    "                ncmp=pd.DataFrame({'Norm':norname,'len':[10,14,20,30][lenidx],'label':label_i,'classifier':clfname,'score':scores })\n",
    "                cmpclassifierdf=cmpclassifierdf.append(ncmp)\n",
    "\n",
    "\n",
    "    #print(\"{0:.3f} (std: {1:.3f})\".format(scores.mean(),np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std\n",
      "raw\n",
      "range\n"
     ]
    }
   ],
   "source": [
    "for norname,DFl in Norm.iteritems():\n",
    "    print norname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f7928106690>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABO8AAAmICAYAAAAqwIoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7928106f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.factorplot(x=\"classifier\", y=\"score\", col='label',row='len',hue='Norm',data=cmpclassifierdf,\n",
    "                   size=6, kind=\"box\",palette=\"muted\")\n",
    "g.set_xticklabels(rotation=30)\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"score\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
